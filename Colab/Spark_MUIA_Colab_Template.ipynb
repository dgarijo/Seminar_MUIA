{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_MUIA_Colab_Template",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO1ctxO3nHYa7QYTf+WJmH9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosafilgueira/Seminar_MUIA/blob/main/Colab/Spark_MUIA_Colab_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning Seminar_MUIA repo"
      ],
      "metadata": {
        "id": "TSNdwvw7bouL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF0x0WUySq63",
        "outputId": "9352e820-0602-4954-f280-c9cc6425ba2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Seminar_MUIA'...\n",
            "remote: Enumerating objects: 449, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 449 (delta 5), reused 4 (delta 1), pack-reused 433\u001b[K\n",
            "Receiving objects: 100% (449/449), 43.54 MiB | 31.14 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n"
          ]
        }
      ],
      "source": [
        " !git clone https://github.com/rosafilgueira/Seminar_MUIA.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Apache Spark"
      ],
      "metadata": {
        "id": "7ZRuUyBLb0pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wbvm4bDS_tO",
        "outputId": "dbf36576-8a12-4498-b4a2-3263755ace13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 45 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 52.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=ff5e4048820b526ba61767ec02241603c07f3d8e6528b2859d0d36a3c02e3efa\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rmGkbg1KRS75",
        "outputId": "12df57bf-e370-42d4-f6de-1d91bb216f35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.2.1-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylAzLtCARtXk",
        "outputId": "4cd5cf53-1287-4c6d-81c9-972c7e81d3fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data   spark-3.2.1-bin-hadoop3.2\n",
            "Seminar_MUIA  spark-3.2.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD_6bJ3-RuH-",
        "outputId": "429e2ffc-e132-4c0b-fc7f-760801a1626d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submiting a Spark Application (spark-submit )"
      ],
      "metadata": {
        "id": "j_jB2FAWRUT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordCount"
      ],
      "metadata": {
        "id": "-7MRbjl-SL09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit /content/Seminar_MUIA/Spark_Applications/wordcount.py sample_data/README.md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p34gMjvRjp2",
        "outputId": "f1cb80f5-074f-46f2-d190-4c57cd136e16"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "22/07/01 10:21:28 INFO SparkContext: Running Spark version 3.2.1\n",
            "22/07/01 10:21:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "22/07/01 10:21:29 INFO ResourceUtils: ==============================================================\n",
            "22/07/01 10:21:29 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "22/07/01 10:21:29 INFO ResourceUtils: ==============================================================\n",
            "22/07/01 10:21:29 INFO SparkContext: Submitted application: Spark Count\n",
            "22/07/01 10:21:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "22/07/01 10:21:29 INFO ResourceProfile: Limiting resource is cpu\n",
            "22/07/01 10:21:29 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "22/07/01 10:21:29 INFO SecurityManager: Changing view acls to: root\n",
            "22/07/01 10:21:29 INFO SecurityManager: Changing modify acls to: root\n",
            "22/07/01 10:21:29 INFO SecurityManager: Changing view acls groups to: \n",
            "22/07/01 10:21:29 INFO SecurityManager: Changing modify acls groups to: \n",
            "22/07/01 10:21:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "22/07/01 10:21:30 INFO Utils: Successfully started service 'sparkDriver' on port 34379.\n",
            "22/07/01 10:21:30 INFO SparkEnv: Registering MapOutputTracker\n",
            "22/07/01 10:21:30 INFO SparkEnv: Registering BlockManagerMaster\n",
            "22/07/01 10:21:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "22/07/01 10:21:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "22/07/01 10:21:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "22/07/01 10:21:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-65efaad8-e1f9-4e1d-a0ca-6b6ce10901b6\n",
            "22/07/01 10:21:30 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "22/07/01 10:21:30 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "22/07/01 10:21:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "22/07/01 10:21:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://0d20b780d8f3:4040\n",
            "22/07/01 10:21:31 INFO Executor: Starting executor ID driver on host 0d20b780d8f3\n",
            "22/07/01 10:21:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46609.\n",
            "22/07/01 10:21:31 INFO NettyBlockTransferService: Server created on 0d20b780d8f3:46609\n",
            "22/07/01 10:21:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "22/07/01 10:21:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0d20b780d8f3, 46609, None)\n",
            "22/07/01 10:21:31 INFO BlockManagerMasterEndpoint: Registering block manager 0d20b780d8f3:46609 with 434.4 MiB RAM, BlockManagerId(driver, 0d20b780d8f3, 46609, None)\n",
            "22/07/01 10:21:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0d20b780d8f3, 46609, None)\n",
            "22/07/01 10:21:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0d20b780d8f3, 46609, None)\n",
            "22/07/01 10:21:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 219.7 KiB, free 434.2 MiB)\n",
            "22/07/01 10:21:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.1 KiB, free 434.2 MiB)\n",
            "22/07/01 10:21:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0d20b780d8f3:46609 (size: 32.1 KiB, free: 434.4 MiB)\n",
            "22/07/01 10:21:32 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "22/07/01 10:21:33 INFO FileInputFormat: Total input files to process : 1\n",
            "22/07/01 10:21:33 INFO SparkContext: Starting job: collect at /content/Seminar_MUIA/Spark_Applications/wordcount.py:10\n",
            "22/07/01 10:21:33 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/Seminar_MUIA/Spark_Applications/wordcount.py:9) as input to shuffle 0\n",
            "22/07/01 10:21:33 INFO DAGScheduler: Got job 0 (collect at /content/Seminar_MUIA/Spark_Applications/wordcount.py:10) with 2 output partitions\n",
            "22/07/01 10:21:33 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /content/Seminar_MUIA/Spark_Applications/wordcount.py:10)\n",
            "22/07/01 10:21:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "22/07/01 10:21:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "22/07/01 10:21:33 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/Seminar_MUIA/Spark_Applications/wordcount.py:9), which has no missing parents\n",
            "22/07/01 10:21:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 434.1 MiB)\n",
            "22/07/01 10:21:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 434.1 MiB)\n",
            "22/07/01 10:21:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0d20b780d8f3:46609 (size: 7.2 KiB, free: 434.4 MiB)\n",
            "22/07/01 10:21:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "22/07/01 10:21:33 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/Seminar_MUIA/Spark_Applications/wordcount.py:9) (first 15 tasks are for partitions Vector(0, 1))\n",
            "22/07/01 10:21:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "22/07/01 10:21:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0d20b780d8f3, executor driver, partition 0, PROCESS_LOCAL, 4493 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:21:33 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (0d20b780d8f3, executor driver, partition 1, PROCESS_LOCAL, 4493 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:21:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "22/07/01 10:21:33 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "22/07/01 10:21:34 INFO HadoopRDD: Input split: file:/content/sample_data/README.md:0+465\n",
            "22/07/01 10:21:34 INFO HadoopRDD: Input split: file:/content/sample_data/README.md:465+465\n",
            "22/07/01 10:21:35 INFO PythonRunner: Times: total = 548, boot = 506, init = 41, finish = 1\n",
            "22/07/01 10:21:35 INFO PythonRunner: Times: total = 550, boot = 502, init = 46, finish = 2\n",
            "22/07/01 10:21:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1569 bytes result sent to driver\n",
            "22/07/01 10:21:35 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1612 bytes result sent to driver\n",
            "22/07/01 10:21:35 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1454 ms on 0d20b780d8f3 (executor driver) (1/2)\n",
            "22/07/01 10:21:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1508 ms on 0d20b780d8f3 (executor driver) (2/2)\n",
            "22/07/01 10:21:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "22/07/01 10:21:35 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50961\n",
            "22/07/01 10:21:35 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/Seminar_MUIA/Spark_Applications/wordcount.py:9) finished in 1.742 s\n",
            "22/07/01 10:21:35 INFO DAGScheduler: looking for newly runnable stages\n",
            "22/07/01 10:21:35 INFO DAGScheduler: running: Set()\n",
            "22/07/01 10:21:35 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "22/07/01 10:21:35 INFO DAGScheduler: failed: Set()\n",
            "22/07/01 10:21:35 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /content/Seminar_MUIA/Spark_Applications/wordcount.py:10), which has no missing parents\n",
            "22/07/01 10:21:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.4 KiB, free 434.1 MiB)\n",
            "22/07/01 10:21:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\n",
            "22/07/01 10:21:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 0d20b780d8f3:46609 (size: 5.5 KiB, free: 434.4 MiB)\n",
            "22/07/01 10:21:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "22/07/01 10:21:35 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /content/Seminar_MUIA/Spark_Applications/wordcount.py:10) (first 15 tasks are for partitions Vector(0, 1))\n",
            "22/07/01 10:21:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "22/07/01 10:21:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (0d20b780d8f3, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:21:35 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (0d20b780d8f3, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:21:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "22/07/01 10:21:35 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
            "22/07/01 10:21:35 INFO ShuffleBlockFetcherIterator: Getting 2 (850.0 B) non-empty blocks including 2 (850.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "22/07/01 10:21:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 74 ms\n",
            "22/07/01 10:21:35 INFO ShuffleBlockFetcherIterator: Getting 2 (532.0 B) non-empty blocks including 2 (532.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "22/07/01 10:21:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 100 ms\n",
            "22/07/01 10:21:35 INFO PythonRunner: Times: total = 28, boot = -876, init = 904, finish = 0\n",
            "22/07/01 10:21:35 INFO PythonRunner: Times: total = 34, boot = -861, init = 894, finish = 1\n",
            "22/07/01 10:21:35 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 2572 bytes result sent to driver\n",
            "22/07/01 10:21:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 0d20b780d8f3:46609 in memory (size: 7.2 KiB, free: 434.4 MiB)\n",
            "22/07/01 10:21:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2176 bytes result sent to driver\n",
            "22/07/01 10:21:35 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 309 ms on 0d20b780d8f3 (executor driver) (1/2)\n",
            "22/07/01 10:21:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 314 ms on 0d20b780d8f3 (executor driver) (2/2)\n",
            "22/07/01 10:21:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "22/07/01 10:21:35 INFO DAGScheduler: ResultStage 1 (collect at /content/Seminar_MUIA/Spark_Applications/wordcount.py:10) finished in 0.358 s\n",
            "22/07/01 10:21:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "22/07/01 10:21:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "22/07/01 10:21:35 INFO DAGScheduler: Job 0 finished: collect at /content/Seminar_MUIA/Spark_Applications/wordcount.py:10, took 2.269585 s\n",
            "directory: 1\n",
            "datasets: 1\n",
            "*: 3\n",
            "`california_housing_data*.csv`: 1\n",
            "is: 4\n",
            "housing: 1\n",
            "1990: 1\n",
            "more: 1\n",
            "at:: 2\n",
            "https://developers.google.com/machine-learning/crash-course/california-housing-data-description: 1\n",
            "`mnist_*.csv`: 1\n",
            "of: 2\n",
            "[Anscombe's: 1\n",
            "was: 2\n",
            "originally: 1\n",
            "in: 2\n",
            "Anscombe,: 1\n",
            "F.: 1\n",
            "'Graphs: 1\n",
            "American: 1\n",
            "Statistician.: 1\n",
            "(1):: 1\n",
            "prepared: 1\n",
            "This: 1\n",
            "includes: 1\n",
            "a: 3\n",
            "few: 1\n",
            "sample: 2\n",
            "to: 1\n",
            "get: 1\n",
            "you: 1\n",
            "started.: 1\n",
            "California: 1\n",
            "data: 1\n",
            "from: 1\n",
            "the: 3\n",
            "US: 1\n",
            "Census;: 1\n",
            "information: 1\n",
            "available: 1\n",
            "small: 1\n",
            "[MNIST: 1\n",
            "database](https://en.wikipedia.org/wiki/MNIST_database),: 1\n",
            "which: 1\n",
            "described: 2\n",
            "http://yann.lecun.com/exdb/mnist/: 1\n",
            "`anscombe.json`: 1\n",
            "contains: 1\n",
            "copy: 2\n",
            "quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet);: 1\n",
            "it: 1\n",
            "J.: 1\n",
            "(1973).: 1\n",
            "Statistical: 1\n",
            "Analysis'.: 1\n",
            "27: 1\n",
            "17-21.: 1\n",
            "JSTOR: 1\n",
            "2682899.: 1\n",
            "and: 1\n",
            "our: 1\n",
            "by: 1\n",
            "[vega_datasets: 1\n",
            "library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).: 1\n",
            "22/07/01 10:21:35 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "22/07/01 10:21:35 INFO SparkUI: Stopped Spark web UI at http://0d20b780d8f3:4040\n",
            "22/07/01 10:21:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "22/07/01 10:21:35 INFO MemoryStore: MemoryStore cleared\n",
            "22/07/01 10:21:35 INFO BlockManager: BlockManager stopped\n",
            "22/07/01 10:21:35 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "22/07/01 10:21:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "22/07/01 10:21:35 INFO SparkContext: Successfully stopped SparkContext\n",
            "22/07/01 10:21:35 INFO ShutdownHookManager: Shutdown hook called\n",
            "22/07/01 10:21:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-a7c99c1f-c319-4a77-b61f-3f231393db22\n",
            "22/07/01 10:21:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-7025a6e5-ffd2-41ed-9bd1-78968b31cec9\n",
            "22/07/01 10:21:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-a7c99c1f-c319-4a77-b61f-3f231393db22/pyspark-e087709a-8c48-4735-a9ce-f22ddeb9073a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SparkPi example "
      ],
      "metadata": {
        "id": "ySmG-J55SVk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --verbose --class org.apache.spark.examples.SparkPi --master local[*] --deploy-mode client $SPARK_HOME/examples/jars/spark-examples_2.12-3.2.1.jar 10 > out.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQBsS73zSADz",
        "outputId": "9b0c4439-c42f-4639-a6ca-5ef9af51cc60"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using properties file: null\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Parsed arguments:\n",
            "  master                  local[*]\n",
            "  deployMode              client\n",
            "  executorMemory          null\n",
            "  executorCores           null\n",
            "  totalExecutorCores      null\n",
            "  propertiesFile          null\n",
            "  driverMemory            null\n",
            "  driverCores             null\n",
            "  driverExtraClassPath    null\n",
            "  driverExtraLibraryPath  null\n",
            "  driverExtraJavaOptions  null\n",
            "  supervise               false\n",
            "  queue                   null\n",
            "  numExecutors            null\n",
            "  files                   null\n",
            "  pyFiles                 null\n",
            "  archives                null\n",
            "  mainClass               org.apache.spark.examples.SparkPi\n",
            "  primaryResource         file:/content/spark-3.2.1-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.2.1.jar\n",
            "  name                    org.apache.spark.examples.SparkPi\n",
            "  childArgs               [10]\n",
            "  jars                    null\n",
            "  packages                null\n",
            "  packagesExclusions      null\n",
            "  repositories            null\n",
            "  verbose                 true\n",
            "\n",
            "Spark properties used, including those specified through\n",
            " --conf and those from the properties file null:\n",
            "  \n",
            "\n",
            "    \n",
            "Main class:\n",
            "org.apache.spark.examples.SparkPi\n",
            "Arguments:\n",
            "10\n",
            "Spark config:\n",
            "(spark.jars,file:/content/spark-3.2.1-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.2.1.jar)\n",
            "(spark.app.name,org.apache.spark.examples.SparkPi)\n",
            "(spark.submit.pyFiles,)\n",
            "(spark.submit.deployMode,client)\n",
            "(spark.master,local[*])\n",
            "Classpath elements:\n",
            "file:/content/spark-3.2.1-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.2.1.jar\n",
            "\n",
            "\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "22/07/01 10:25:47 INFO SparkContext: Running Spark version 3.2.1\n",
            "22/07/01 10:25:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "22/07/01 10:25:47 INFO ResourceUtils: ==============================================================\n",
            "22/07/01 10:25:47 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "22/07/01 10:25:47 INFO ResourceUtils: ==============================================================\n",
            "22/07/01 10:25:47 INFO SparkContext: Submitted application: Spark Pi\n",
            "22/07/01 10:25:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "22/07/01 10:25:47 INFO ResourceProfile: Limiting resource is cpu\n",
            "22/07/01 10:25:47 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "22/07/01 10:25:47 INFO SecurityManager: Changing view acls to: root\n",
            "22/07/01 10:25:47 INFO SecurityManager: Changing modify acls to: root\n",
            "22/07/01 10:25:47 INFO SecurityManager: Changing view acls groups to: \n",
            "22/07/01 10:25:47 INFO SecurityManager: Changing modify acls groups to: \n",
            "22/07/01 10:25:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "22/07/01 10:25:48 INFO Utils: Successfully started service 'sparkDriver' on port 44759.\n",
            "22/07/01 10:25:48 INFO SparkEnv: Registering MapOutputTracker\n",
            "22/07/01 10:25:48 INFO SparkEnv: Registering BlockManagerMaster\n",
            "22/07/01 10:25:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "22/07/01 10:25:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "22/07/01 10:25:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "22/07/01 10:25:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5fbb0ec8-f8b3-4ac0-8271-fc8b7bf0aec9\n",
            "22/07/01 10:25:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "22/07/01 10:25:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "22/07/01 10:25:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "22/07/01 10:25:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://0d20b780d8f3:4040\n",
            "22/07/01 10:25:49 INFO SparkContext: Added JAR file:/content/spark-3.2.1-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.2.1.jar at spark://0d20b780d8f3:44759/jars/spark-examples_2.12-3.2.1.jar with timestamp 1656671147361\n",
            "22/07/01 10:25:49 INFO Executor: Starting executor ID driver on host 0d20b780d8f3\n",
            "22/07/01 10:25:49 INFO Executor: Fetching spark://0d20b780d8f3:44759/jars/spark-examples_2.12-3.2.1.jar with timestamp 1656671147361\n",
            "22/07/01 10:25:49 INFO TransportClientFactory: Successfully created connection to 0d20b780d8f3/172.28.0.2:44759 after 61 ms (0 ms spent in bootstraps)\n",
            "22/07/01 10:25:50 INFO Utils: Fetching spark://0d20b780d8f3:44759/jars/spark-examples_2.12-3.2.1.jar to /tmp/spark-7aa2e8dd-e8d4-43c0-9451-f2c8fa1215d7/userFiles-32b1c5a3-784a-462e-8b8e-c83426e1ac3d/fetchFileTemp15925692094346270657.tmp\n",
            "22/07/01 10:25:50 INFO Executor: Adding file:/tmp/spark-7aa2e8dd-e8d4-43c0-9451-f2c8fa1215d7/userFiles-32b1c5a3-784a-462e-8b8e-c83426e1ac3d/spark-examples_2.12-3.2.1.jar to class loader\n",
            "22/07/01 10:25:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37577.\n",
            "22/07/01 10:25:50 INFO NettyBlockTransferService: Server created on 0d20b780d8f3:37577\n",
            "22/07/01 10:25:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "22/07/01 10:25:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0d20b780d8f3, 37577, None)\n",
            "22/07/01 10:25:50 INFO BlockManagerMasterEndpoint: Registering block manager 0d20b780d8f3:37577 with 434.4 MiB RAM, BlockManagerId(driver, 0d20b780d8f3, 37577, None)\n",
            "22/07/01 10:25:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0d20b780d8f3, 37577, None)\n",
            "22/07/01 10:25:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0d20b780d8f3, 37577, None)\n",
            "22/07/01 10:25:51 INFO SparkContext: Starting job: reduce at SparkPi.scala:38\n",
            "22/07/01 10:25:51 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions\n",
            "22/07/01 10:25:51 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
            "22/07/01 10:25:51 INFO DAGScheduler: Parents of final stage: List()\n",
            "22/07/01 10:25:51 INFO DAGScheduler: Missing parents: List()\n",
            "22/07/01 10:25:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
            "22/07/01 10:25:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 434.4 MiB)\n",
            "22/07/01 10:25:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 434.4 MiB)\n",
            "22/07/01 10:25:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0d20b780d8f3:37577 (size: 2.3 KiB, free: 434.4 MiB)\n",
            "22/07/01 10:25:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478\n",
            "22/07/01 10:25:51 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
            "22/07/01 10:25:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks resource profile 0\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0d20b780d8f3, executor driver, partition 0, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (0d20b780d8f3, executor driver, partition 1, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "22/07/01 10:25:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (0d20b780d8f3, executor driver, partition 2, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 312 ms on 0d20b780d8f3 (executor driver) (1/10)\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (0d20b780d8f3, executor driver, partition 3, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 404 ms on 0d20b780d8f3 (executor driver) (2/10)\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (0d20b780d8f3, executor driver, partition 4, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 68 ms on 0d20b780d8f3 (executor driver) (3/10)\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (0d20b780d8f3, executor driver, partition 5, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (0d20b780d8f3, executor driver, partition 6, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 86 ms on 0d20b780d8f3 (executor driver) (4/10)\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 101 ms on 0d20b780d8f3 (executor driver) (5/10)\n",
            "22/07/01 10:25:52 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (0d20b780d8f3, executor driver, partition 7, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 80 ms on 0d20b780d8f3 (executor driver) (6/10)\n",
            "22/07/01 10:25:52 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (0d20b780d8f3, executor driver, partition 8, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 108 ms on 0d20b780d8f3 (executor driver) (7/10)\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (0d20b780d8f3, executor driver, partition 9, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
            "22/07/01 10:25:52 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 98 ms on 0d20b780d8f3 (executor driver) (8/10)\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 72 ms on 0d20b780d8f3 (executor driver) (9/10)\n",
            "22/07/01 10:25:52 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 957 bytes result sent to driver\n",
            "22/07/01 10:25:52 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 38 ms on 0d20b780d8f3 (executor driver) (10/10)\n",
            "22/07/01 10:25:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "22/07/01 10:25:52 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 1.136 s\n",
            "22/07/01 10:25:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "22/07/01 10:25:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "22/07/01 10:25:52 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 1.268450 s\n",
            "22/07/01 10:25:52 INFO SparkUI: Stopped Spark web UI at http://0d20b780d8f3:4040\n",
            "22/07/01 10:25:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "22/07/01 10:25:52 INFO MemoryStore: MemoryStore cleared\n",
            "22/07/01 10:25:52 INFO BlockManager: BlockManager stopped\n",
            "22/07/01 10:25:52 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "22/07/01 10:25:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "22/07/01 10:25:52 INFO SparkContext: Successfully stopped SparkContext\n",
            "22/07/01 10:25:52 INFO ShutdownHookManager: Shutdown hook called\n",
            "22/07/01 10:25:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-6166ef00-ecd7-443a-a169-a7ea61892813\n",
            "22/07/01 10:25:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-7aa2e8dd-e8d4-43c0-9451-f2c8fa1215d7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!more out.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynugHOrtS7rD",
        "outputId": "10e5430c-5c94-4604-cf15-f12bc1ff4cec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi is roughly 3.1413751413751414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an Apache Spark session"
      ],
      "metadata": {
        "id": "IgsHMQwEcK48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "Y7gaimijcMPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "YmBLB12fcX_p",
        "outputId": "2b259750-71a6-4890-b55d-090e28e23292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f064cddbc10>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3186b2fad2ed:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Learning_Spark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a sparkContext"
      ],
      "metadata": {
        "id": "osFJwSj3c28h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "YcFWUe8jcZXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path to dataset (inside Seminar_MUIA)"
      ],
      "metadata": {
        "id": "zoAzFByKc-Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "data_file=\"/content/Seminar_MUIA/walkthrough_examples/jadianes-spark-py-notebooks/kddcup.data_10_percent.gz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM8q8ETkTUYt",
        "outputId": "2b60e34f-17bf-47e1-eda1-eef3ee752cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = sc.textFile(data_file)"
      ],
      "metadata": {
        "id": "HDuvUZYDdPkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOwmiQMbdioW",
        "outputId": "802f9880-01e0-4adf-ae36-28a44307f745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "494021"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}