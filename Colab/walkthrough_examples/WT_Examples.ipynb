{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosafilgueira/Seminar_MUIA/blob/main/Colab/walkthrough_examples/WT_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Walkthrough_examples "
      ],
      "metadata": {
        "id": "Iw_Vo3mbrqB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the enviroment"
      ],
      "metadata": {
        "id": "STTF9_xqr0vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rosafilgueira/Seminar_MUIA.git\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "data_path=\"/content/Seminar_MUIA/walkthrough_examples/jadianes-spark-py-notebooks/\""
      ],
      "metadata": {
        "id": "5YsDex4qr3Ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8616a61-6b51-4b1f-9e0d-af7d5d653d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Seminar_MUIA'...\n",
            "remote: Enumerating objects: 375, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 375 (delta 81), reused 64 (delta 24), pack-reused 223\u001b[K\n",
            "Receiving objects: 100% (375/375), 44.15 MiB | 22.95 MiB/s, done.\n",
            "Resolving deltas: 100% (180/180), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 46 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 51.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=2bb7c33fe70491a363358625965cc71857e3bfe27cee47ccc43c734fcbbab11e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jTDopxNqz9k"
      },
      "source": [
        "# RDD creation - Walkthrough Example 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Nl40Waqz9l"
      },
      "source": [
        "#### [Introduction to Spark with Python]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4zdgJ5rqz9m"
      },
      "source": [
        "In this notebook we will introduce two different ways of getting data into the basic Spark data structure, the **Resilient Distributed Dataset** or **RDD**. An RDD is a distributed collection of elements. All work in Spark is expressed as either creating new RDDs, transforming existing RDDs, or calling actions on RDDs to compute a result. Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSF1O23wqz9n"
      },
      "source": [
        "#### References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGZgGKBuqz9o"
      },
      "source": [
        "The KDD Cup 1999 competition dataset is described in detail [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-OMCNanqz9o"
      },
      "source": [
        "## Getting the data files  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73e50h8Xqz9o"
      },
      "source": [
        "The reference book for these and other Spark related topics is *Learning Spark* by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofxt_QDlqz9p"
      },
      "source": [
        "## Creating a RDD from a file  \n",
        "#### KDDCUP99\n",
        "This is the data set used for The Third International Knowledge Discovery and Data Mining Tools Competition, which was held in conjunction with KDD-99 The Fifth International Conference on Knowledge Discovery and Data Mining. The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between ``bad'' connections, called intrusions or attacks, and ``good'' normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment.\n",
        "\n",
        " This is the original task decription given to competition participants (http://kdd.ics.uci.edu/databases/kddcup99/task.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBgGpuRGqz9p"
      },
      "outputs": [],
      "source": [
        "data_file = data_path+\"kddcup.data_10_percent.gz\"\n",
        "raw_data = sc.textFile(data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc_wfjQYqz9r"
      },
      "source": [
        "In this notebook we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a *Gzip* file that we will download locally.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRwvb63rqz9r"
      },
      "source": [
        "Now we have our data file loaded into the `raw_data` RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wHKhq5qz9s"
      },
      "source": [
        "Without getting into Spark *transformations* and *actions*, the most basic thing we can do to check that we got our RDD contents right is to `count()` the number of lines loaded from the file into the RDD.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbQ9-526qz9s"
      },
      "outputs": [],
      "source": [
        "raw_data.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9tkh2urqz9t"
      },
      "source": [
        "We can also check the first few entries in our data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2yNwlHXqz9t"
      },
      "outputs": [],
      "source": [
        "raw_data.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2oyVsL1qz9u"
      },
      "source": [
        "## Filter() and Count()\n",
        "\n",
        "Count the number of 'normal.'  interacions we have in our dataset, using filter() transformation and count actions(). \n",
        "\n",
        "Remeber that a function is evaluated on every element in the original RDD. The new resulting RDD will contain just those elements that make the function return `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFc5HspMqz9u"
      },
      "outputs": [],
      "source": [
        "goodLines = raw_data.filter(lambda x: 'normal.' in x)\n",
        "goodLines.count()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrN76FChqz9u"
      },
      "source": [
        "### Map()\n",
        "\n",
        "By using the `map` transformation in Spark, we can apply a function to every element in our RDD. Python's lambdas are specially expressive for this particular. In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hac9a_nqz9v"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
        "t0 = time()\n",
        "head_rows = csv_data.take(5)\n",
        "tt = time() - t0\n",
        "print (\"Parse completed\", tt) \n",
        "print(head_rows[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGJtLR85qz9v"
      },
      "source": [
        "## Creating and RDD using `parallelize`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdO0SVKvqz9v"
      },
      "source": [
        "Another way of creating an RDD is to parallelize an already existing list.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_mcqSrEqz9v"
      },
      "outputs": [],
      "source": [
        "a = range(100)\n",
        "\n",
        "data = sc.parallelize(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqJtvOb0qz9v"
      },
      "source": [
        "As we did before, we can `count()` the number of elements in the RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un1Ptzqoqz9w"
      },
      "outputs": [],
      "source": [
        "data.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsi7rvgrqz9w"
      },
      "source": [
        "As before, we can access the first few elements on our RDD.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz2xyDhXqz9w"
      },
      "outputs": [],
      "source": [
        "data.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkzbsu2L8YWe"
      },
      "source": [
        "# Sampling RDDs - Walkthrough Example 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlmweF_V8YWf"
      },
      "source": [
        "So far we have introduced RDD creation together with some basic transformations such as `map` and `filter` and some actions such as `count`, `take`, and `collect`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F5gp4zR8YWg"
      },
      "source": [
        "This notebook will show how to sample RDDs. Regarding transformations, `sample` will be introduced since it will be useful in many statistical learning scenarios. Then we will compare results with the `takeSample` action.      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XYCWS7k8YWg"
      },
      "source": [
        "## Getting the data and creating the RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW590pry8YWg"
      },
      "outputs": [],
      "source": [
        "data_file = data_file = data_path+\"kddcup.data.gz\"\n",
        "raw_data = sc.textFile(data_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wv4TQF5K89oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jEjtJLs8YWh"
      },
      "source": [
        "## Sampling RDDs   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK32zPMT8YWi"
      },
      "source": [
        "In Spark, there are two sampling operations, the transformation `sample` and the action `takeSample`. By using a transformation we can tell Spark to apply successive transformation on a sample of a given RDD. By using an action we retrieve a given sample and we can have it in local memory to be used by any other standard library (e.g. Scikit-learn).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzR2r9J-8YWi"
      },
      "source": [
        "### The `sample` transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeDUhnxk8YWj"
      },
      "source": [
        "The `sample` transformation takes up to three parameters. First is whether the sampling is done with replacement or not. Second is the sample size as a fraction. Finally we can optionally provide a *random seed*.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM56Kmkf8YWj",
        "outputId": "b17c818f-4b52-4dff-8c29-802decbbc15e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 1:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample size is 489957 of 4898431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "raw_data_sample = raw_data.sample(False, 0.1, 1234)\n",
        "sample_size = raw_data_sample.count()\n",
        "total_size = raw_data.count()\n",
        "print (\"Sample size is {} of {}\" .format(sample_size, total_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6lMXaOr8YWk"
      },
      "source": [
        "But the power of sampling as a transformation comes from doing it as part of a sequence of additional transformations. This will show more powerful once we start doing aggregations and key-value pairs operations, and will be specially useful when using Spark's machine learning library MLlib.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCjlH85S8YWl"
      },
      "source": [
        "In the meantime, imagine we want to have an approximation of the proportion of `normal.` interactions in our dataset. We could do this by counting the total number of tags as we did in previous notebooks. However we want a quicker response and we don't need the exact answer but just an approximation. We can do it as follows.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JZzHA5a8YWl",
        "outputId": "8c85872b-d5a5-471e-f447-626cfd8e1a01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 2:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The ratio of 'normal' interactions is 0.199\n",
            "Count done in 4.252 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "\n",
        "# transformations to be applied\n",
        "raw_data_sample_items = raw_data_sample.map(lambda x: x.split(\",\"))\n",
        "sample_normal_tags = raw_data_sample_items.filter(lambda x: \"normal.\" in x)\n",
        "\n",
        "# actions + time\n",
        "t0 = time()\n",
        "sample_normal_tags_count = sample_normal_tags.count()\n",
        "tt = time() - t0\n",
        "\n",
        "sample_normal_ratio = sample_normal_tags_count / float(sample_size)\n",
        "print (\"The ratio of 'normal' interactions is {}\".format(round(sample_normal_ratio,3))) \n",
        "print (\"Count done in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GWTF-rd8YWm"
      },
      "source": [
        "Let's compare this with calculating the ratio without sampling.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsnyDLGy8YWm",
        "outputId": "8fb78eda-d846-432e-cbc9-03666a4f0b9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 3:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The ratio of 'normal' interactions is 0.199\n",
            "Count done in 10.312 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# transformations to be applied\n",
        "raw_data_items = raw_data.map(lambda x: x.split(\",\"))\n",
        "normal_tags = raw_data_items.filter(lambda x: \"normal.\" in x)\n",
        "\n",
        "# actions + time\n",
        "t0 = time()\n",
        "normal_tags_count = normal_tags.count()\n",
        "tt = time() - t0\n",
        "\n",
        "normal_ratio = normal_tags_count / float(total_size)\n",
        "print (\"The ratio of 'normal' interactions is {}\".format(round(normal_ratio,3))) \n",
        "print (\"Count done in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt5Rhlhv8YWm"
      },
      "source": [
        "We can see a gain in time. The more transformations we apply after the sampling the bigger this gain. This is because without sampling all the transformations are applied to the complete set of data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSBAyMe68YWn"
      },
      "source": [
        "### The `takeSample` action  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhU_-pKk8YWn"
      },
      "source": [
        "If what we need is to grab a sample of raw data from our RDD into local memory in order to be used by other non-Spark libraries, `takeSample` can be used.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G9trXHp8YWn"
      },
      "source": [
        "The syntax is very similar, but in this case we specify the number of items instead of the sample size as a fraction of the complete data size.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO_auxro8YWn",
        "outputId": "e35f73ca-d4c2-4272-dede-1b3ad4e94be3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The ratio of 'normal' interactions is 0.1988025\n",
            "Count done in 6.962 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/06/24 12:38:54 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 122274 ms exceeds timeout 120000 ms\n",
            "22/06/24 12:38:54 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
          ]
        }
      ],
      "source": [
        "t0 = time()\n",
        "raw_data_sample = raw_data.takeSample(False, 400000, 1234)\n",
        "normal_data_sample = [x.split(\",\") for x in raw_data_sample if \"normal.\" in x]\n",
        "tt = time() - t0\n",
        "\n",
        "normal_sample_size = len(normal_data_sample)\n",
        "\n",
        "normal_ratio = normal_sample_size / 400000.0\n",
        "print (\"The ratio of 'normal' interactions is {}\".format(normal_ratio))\n",
        "print (\"Count done in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk61xqWK8YWo"
      },
      "source": [
        "The process was very similar as before. We obtained a sample of about 10 percent of the data, and then filter and split.  \n",
        "\n",
        "However, it took longer, even with a slightly smaller sample. The reason is that Spark just distributed the execution of the sampling process. The filtering and splitting of the results were done locally in a single node.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks9Dxahx9SUs"
      },
      "source": [
        "# Set operations on RDDs - Walkthrough Example 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YpD8DO79SUt"
      },
      "source": [
        "Spark supports many of the operations we have in mathematical sets, such as union and intersection, even when the RDDs themselves are not properly sets. It is important to note that these operations require that the RDDs being operated on are of the same type.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvt_mSyX9SUt"
      },
      "source": [
        "Set operations are quite straightforward to understand as it work as expected. The only consideration comes from the fact that RDDs are not real sets, and therefore operations such as the union of RDDs doesn't remove duplicates. In this notebook we will have a brief look at `subtract`, `distinct`, and `cartesian`.       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Mc14pY9SUu"
      },
      "source": [
        "## Getting the data and creating the RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyxfOB9k9SUv"
      },
      "source": [
        "As we did in our first notebook, we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a Gzip file that we will download locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPzREEek9SUv"
      },
      "outputs": [],
      "source": [
        "data_file = data_path+ \"kddcup.data_10_percent.gz\"\n",
        "raw_data = sc.textFile(data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ6gEbwZ9SUw"
      },
      "source": [
        "## Getting attack interactions using `subtract`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0GCXBQV9SUw"
      },
      "source": [
        "For illustrative purposes, imagine we already have our RDD with non attack (normal) interactions from some previous analysis.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzlmKyr79SUw"
      },
      "outputs": [],
      "source": [
        "normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdB5BFod9SUx"
      },
      "source": [
        "We can obtain attack interactions by subtracting normal ones from the original unfiltered RDD as follows.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HseMIqcw9SUx"
      },
      "outputs": [],
      "source": [
        "attack_raw_data = raw_data.subtract(normal_raw_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-MMVruS9SUx"
      },
      "source": [
        "Let's do some counts to check our results.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e-IILnk9SUx"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "# count all\n",
        "t0 = time()\n",
        "raw_data_count = raw_data.count()\n",
        "tt = time() - t0\n",
        "print (\"All count in {} secs\" .format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX4DKFjs9SUx"
      },
      "outputs": [],
      "source": [
        "# count normal\n",
        "t0 = time()\n",
        "normal_raw_data_count = normal_raw_data.count()\n",
        "tt = time() - t0\n",
        "print (\"Normal count in {} secs\" .format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BXAg_ho9SUx"
      },
      "outputs": [],
      "source": [
        "# count attacks\n",
        "t0 = time()\n",
        "attack_raw_data_count = attack_raw_data.count()\n",
        "tt = time() - t0\n",
        "print (\"Attack count in {} secs\" .format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db5iL6yp9SUx"
      },
      "outputs": [],
      "source": [
        "print (\"There are {} normal interactions and {} attacks, \\\n",
        "from a total of {} interactions\" .format(normal_raw_data_count,attack_raw_data_count,raw_data_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTs9ipe9SUy"
      },
      "source": [
        "So now we have two RDDs, one with normal interactions and another one with attacks.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzT1bKuP9SUy"
      },
      "source": [
        "## Protocol and service combinations using `cartesian`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acN3KB7z9SUy"
      },
      "source": [
        "We can compute the Cartesian product between two RDDs by using the `cartesian` transformation. It returns all possible pairs of elements between two RDDs. In our case we will use it to generate all the possible combinations between service and protocol in our network interactions.  \n",
        "\n",
        "First of all we need to isolate each collection of values in two separate RDDs. For that we will use `distinct` on the CSV-parsed dataset. From the [dataset description](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names) we know that protocol is the second column and service is the third (tag is the last one and not the first as appears in the page).   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO76E_YN9SUy"
      },
      "source": [
        "So first, let's get the protocols.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhYmfoti9SUy"
      },
      "outputs": [],
      "source": [
        "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
        "protocols = csv_data.map(lambda x: x[1]).distinct()\n",
        "protocols.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LWtW8K89SUz"
      },
      "source": [
        "Now we do the same for services.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNxGHF3d9SU0"
      },
      "outputs": [],
      "source": [
        "services = csv_data.map(lambda x: x[2]).distinct()\n",
        "services.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7oIzloF9SU0"
      },
      "source": [
        "A longer list in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR2zpjbU9SU0"
      },
      "source": [
        "Now we can do the cartesian product.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVsjG8ep9SU0"
      },
      "outputs": [],
      "source": [
        "product = protocols.cartesian(services).collect()\n",
        "print ( \"There are {} combinations of protocol X service\" .format(len(product)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7rC3bYT9SU0"
      },
      "source": [
        "Obviously, for such small RDDs doesn't really make sense to use Spark cartesian product. We could have perfectly collected the values after using `distinct` and do the cartesian product locally. Moreover, `distinct` and `cartesian` are expensive operations so they must be used with care when the operating datasets are large.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CttoTrck90lH"
      },
      "source": [
        "# Data aggregations on RDDs - Walkthrough Example 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMome_Df90lI"
      },
      "source": [
        "We can aggregate RDD data in Spark by using three different actions: `reduce`, `fold`, and `aggregate`. The last one is the more general one and someway includes the first two.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAuqvsZY90lI"
      },
      "source": [
        "## Getting the data and creating the RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GEX2rvf90lJ"
      },
      "source": [
        "As we did in our first notebook, we will use the reduced dataset (10 percent) provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million nework interactions. The file is provided as a Gzip file that we will download locally.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO3dpnmx90lJ"
      },
      "outputs": [],
      "source": [
        "data_file = data_path+ \"kddcup.data_10_percent.gz\"\n",
        "raw_data = sc.textFile(data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO4u128490lK"
      },
      "source": [
        "## Inspecting interaction duration by tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSrJC1xH90lK"
      },
      "source": [
        "Both `fold` and `reduce` take a function as an argument that is applied to two elements of the RDD. The `fold` action differs from `reduce` in that it gets and additional initial *zero value* to be used for the initial call. This value should be the identity element for the function provided.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apm3dVsu90lK"
      },
      "source": [
        "As an example, imagine we want to know the total duration of our interactions for normal and attack interactions. We can use `reduce` as follows.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoXBEV3990lL"
      },
      "outputs": [],
      "source": [
        "# parse data\n",
        "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
        "\n",
        "# separate into different RDDs\n",
        "normal_csv_data = csv_data.filter(lambda x: x[41]==\"normal.\")\n",
        "attack_csv_data = csv_data.filter(lambda x: x[41]!=\"normal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIZKex4c90lM"
      },
      "source": [
        "The function that we pass to `reduce` gets and returns elements of the same type of the RDD. If we want to sum durations we need to extract that element into a new RDD.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m95dkqQY90lM"
      },
      "outputs": [],
      "source": [
        "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\n",
        "attack_duration_data = attack_csv_data.map(lambda x: int(x[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPI_nzC790lM"
      },
      "source": [
        "Now we can reduce these new RDDs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nImxyDos90lM"
      },
      "outputs": [],
      "source": [
        "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\n",
        "total_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n",
        "\n",
        "print (\"Total duration for 'normal' interactions is {}\".\\\n",
        "    format(total_normal_duration))\n",
        "print (\"Total duration for 'attack' interactions is {}\".\\\n",
        "    format(total_attack_duration))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai11a-yp90lN"
      },
      "source": [
        "We can go further and use counts to calculate duration means.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhDnPTOL90lN"
      },
      "outputs": [],
      "source": [
        "normal_count = normal_duration_data.count()\n",
        "attack_count = attack_duration_data.count()\n",
        "\n",
        "print (\"Mean duration for 'normal' interactions is {}\".\\\n",
        "    format(round(total_normal_duration/float(normal_count),3)))\n",
        "print (\"Mean duration for 'attack' interactions is {}\".\\\n",
        "    format(round(total_attack_duration/float(attack_count),3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy3VEecg90lN"
      },
      "source": [
        "We have a first (and too simplistic) approach to identify attack interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OcCGs0E90lO"
      },
      "source": [
        "## A better way, using `aggregate`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPO8PNqO90lO"
      },
      "source": [
        "The `aggregate` action frees us from the constraint of having the return be the same type as the RDD we are working on. Like with `fold`, we supply an initial zero value of the type we want to return. Then we provide two functions. The first one is used to combine the elements from our RDD with the accumulator. The second function is needed to merge two accumulators. Let's see it in action calculating the mean we did before.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOlxEbYR90lO"
      },
      "outputs": [],
      "source": [
        "normal_sum_count = normal_duration_data.aggregate(\n",
        "    (0,0), # the initial value\n",
        "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
        "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
        ")\n",
        "\n",
        "print (\"Mean duration for 'normal' interactions is {}\".\\\n",
        "    format(round(normal_sum_count[0]/float(normal_sum_count[1]),3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdS8og-290lO"
      },
      "source": [
        "In the previous aggregation, the accumulator first element keeps the total sum, while the second element keeps the count. Combining an accumulator with an RDD element consists in summing up the value and incrementing the count. Combining two accumulators requires just a pairwise sum.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdX895UU90lO"
      },
      "source": [
        "We can do the same with attack type interactions.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IjJLie_90lO"
      },
      "outputs": [],
      "source": [
        "attack_sum_count = attack_duration_data.aggregate(\n",
        "    (0,0), # the initial value\n",
        "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
        "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
        ")\n",
        "\n",
        "print (\"Mean duration for 'attack' interactions is {}\".\\\n",
        "    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7b7vVkq90lO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "squCn3CM-Fhp"
      },
      "source": [
        "# Working with key/value pair RDDs - Walkthrough Example 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZfR2eDn-Fhq"
      },
      "source": [
        "Spark provides specific functions to deal with RDDs which elements are key/value pairs. They are usually used to perform aggregations and other processings by key.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqMyHsIQ-Fhr"
      },
      "source": [
        "In this notebook we will show how, by working with key/value pairs, we can process our network interactions dataset in a more practical and powerful way than that used in previous notebooks. Key/value pair aggregations will show to be particularly effective when trying to explore each type of tag in our network attacks, in an individual way.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCoz9OrD-Fhs"
      },
      "source": [
        "## Getting the data and creating the RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH6v_-H4-Fhs"
      },
      "outputs": [],
      "source": [
        "data_file = data_path+ \"kddcup.data_10_percent.gz\"\n",
        "raw_data = sc.textFile(data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8eDHdkB-Fht"
      },
      "source": [
        "## Creating a pair RDD for interaction types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU456RR9-Fht"
      },
      "source": [
        "In this notebook we want to do some exploratory data analysis on our network interactions dataset. More concretely we want to profile each network interaction type in terms of some of its variables such as duration. In order to do so, we first need to create the RDD suitable for that, where each interaction is parsed as a CSV row representing the value, and is put together with its corresponding tag as a key.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsQuhPQt-Fhu"
      },
      "source": [
        "Normally we create key/value pair RDDs by applying a function using `map` to the original data. This function returns the corresponding pair for a given RDD element. We can proceed as follows.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh40MDlw-Fhu"
      },
      "outputs": [],
      "source": [
        "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
        "key_value_data = csv_data.map(lambda x: (x[41], x)) # x[41] contains the network interaction tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_35PQyC-Fhu"
      },
      "source": [
        "We have now our key/value pair data ready to be used. Let's get the first element in order to see how it looks like.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgL3VcFe-Fhv"
      },
      "outputs": [],
      "source": [
        "key_value_data.take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhDrT7NA-Fhv"
      },
      "source": [
        "## Data aggregations with key/value pair RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo8pAr1j-Fhv"
      },
      "source": [
        "We can use all the transformations and actions available for normal RDDs with key/value pair RDDs. We just need to make the functions work with pair elements. Additionally, Spark provides specific functions to work with RDDs containing pair elements. They are very similar to those available for general RDDs.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN4BFwEW-Fhv"
      },
      "source": [
        "For example, we have a `reduceByKey` transformation that we can use as follows to calculate the total duration of each network interaction type.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjK8q3mp-Fhw"
      },
      "outputs": [],
      "source": [
        "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) \n",
        "durations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "durations_by_key.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY2scV9G-Fhw"
      },
      "source": [
        "We have a specific counting action for key/value pairs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKDbXwld-Fhw"
      },
      "outputs": [],
      "source": [
        "counts_by_key = key_value_data.countByKey()\n",
        "counts_by_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R4hOoiF-Fhw"
      },
      "source": [
        "### Using `combineByKey`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN2k9eu1-Fhx"
      },
      "source": [
        "This is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it. We can think about it as the `aggregate` equivalent since it allows the user to return values that are not the same type as our input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7_0C2ze-Fhx"
      },
      "source": [
        "For example, we can use it to calculate per-type average durations as follows.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkG-Cu39-Fhx"
      },
      "outputs": [],
      "source": [
        "sum_counts = key_value_duration.combineByKey(\n",
        "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
        "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
        "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
        ")\n",
        "\n",
        "sum_counts.collectAsMap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHcnXIt2-Fhx"
      },
      "source": [
        "We can see that the arguments are pretty similar to those passed to `aggregate` in the previous notebook. The result associated to each type is in the form of a pair. If we want to actually get the averages, we need to do the division before collecting the results.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NLd2C1S-Fhx"
      },
      "outputs": [],
      "source": [
        "#duration_means_by_type = sum_counts.map(lambda (key,value): (key, round(value[0]/value[1],3))).collectAsMap()\n",
        "duration_means_by_type = sum_counts.map(lambda x: (x[0], round(x[1][0]/x[1][1],3))).collectAsMap()\n",
        "\n",
        "# Print them sorted\n",
        "for tag in sorted(duration_means_by_type, key=duration_means_by_type.get, reverse=True):\n",
        "    print (tag, duration_means_by_type[tag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk4zU0Er-Fhy"
      },
      "source": [
        "A small step into understanding what makes a network interaction be considered an attack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY4oMUHJ-ZvD"
      },
      "source": [
        "# MLlib: Basic Statistics and Exploratory Data Analysis - Walkthrough Example 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCyXMhTc-ZvE"
      },
      "source": [
        "So far we have used different map and aggregation functions, on simple and key/value pair RDD's, in order to get simple statistics that help us understand our datasets. In this notebook we will introduce Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) through its basic statistics functionality in order to better understand our dataset. We will use the reduced 10-percent [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) datasets through the notebook.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIzIJHjS-ZvF"
      },
      "source": [
        "## Getting the data and creating the RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GktkzGQ0-ZvF"
      },
      "outputs": [],
      "source": [
        "data_file = data_path+ \"kddcup.data_10_percent.gz\"\n",
        "raw_data = sc.textFile(data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTiSrRt--ZvG"
      },
      "source": [
        "## Local vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChrAYf9n-ZvG"
      },
      "source": [
        "A [local vector](https://spark.apache.org/docs/latest/mllib-data-types.html#local-vector) is often used as a base type for RDDs in Spark MLlib. A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIaXcMRc-ZvH"
      },
      "source": [
        "For dense vectors, MLlib uses either Python *lists* or the *NumPy* `array` type. The later is recommended, so you can simply pass NumPy arrays around.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaooXrbb-ZvH"
      },
      "source": [
        "For sparse vectors, users can construct a `SparseVector` object from MLlib or pass *SciPy* `scipy.sparse` column vectors if SciPy is available in their environment. The easiest way to create sparse vectors is to use the factory methods implemented in `Vectors`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKCsc_97-ZvH"
      },
      "source": [
        "### An RDD of dense vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRyAkGqm-ZvI"
      },
      "source": [
        "Let's represent each network interaction in our dataset as a dense vector. For that we will use the *NumPy* `array` type.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT2e1E1J-ZvI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def parse_interaction(line):\n",
        "    line_split = line.split(\",\")\n",
        "    # keep just numeric and logical values\n",
        "    symbolic_indexes = [1,2,3,41]\n",
        "    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n",
        "    return np.array([float(x) for x in clean_line_split])\n",
        "\n",
        "vector_data = raw_data.map(parse_interaction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFE_wVsx-ZvJ"
      },
      "source": [
        "## Summary statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txgOep1Z-ZvJ"
      },
      "source": [
        "Spark's MLlib provides column summary statistics for `RDD[Vector]` through the function [`colStats`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics.colStats) available in [`Statistics`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics). The method returns an instance of [`MultivariateStatisticalSummary`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.MultivariateStatisticalSummary), which contains the column-wise *max*, *min*, *mean*, *variance*, and *number of nonzeros*, as well as the *total count*.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa0zXRvm-ZvJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.stat import Statistics \n",
        "from math import sqrt \n",
        "\n",
        "# Compute column summary statistics.\n",
        "summary = Statistics.colStats(vector_data)\n",
        "\n",
        "print (\"Duration Statistics:\")\n",
        "print (\" Mean: {}\".format(round(summary.mean()[0],3)))\n",
        "print (\" St. deviation: {}\".format(round(sqrt(summary.variance()[0]),3)))\n",
        "print (\" Max value: {}\".format(round(summary.max()[0],3)))\n",
        "print (\" Min value: {}\".format(round(summary.min()[0],3)))\n",
        "print (\" Total value count: {}\".format(summary.count()))\n",
        "print (\" Number of non-zero values: {}\".format(summary.numNonzeros()[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KXmnNc1-ZvK"
      },
      "source": [
        "The interesting part of summary statistics, in our case, comes from being able to obtain them by the type of network attack or 'label' in our dataset. By doing so we will be able to better characterise our dataset dependent variable in terms of the independent variables range of values.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiWp6nZ-ZvK"
      },
      "source": [
        "If we want to do such a thing we could filter our RDD containing labels as keys and vectors as values. For that we just need to adapt our `parse_interaction` function to return a tuple with both elements.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hTshhiK-ZvK"
      },
      "outputs": [],
      "source": [
        "def parse_interaction_with_key(line):\n",
        "    line_split = line.split(\",\")\n",
        "    # keep just numeric and logical values\n",
        "    symbolic_indexes = [1,2,3,41]\n",
        "    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n",
        "    return (line_split[41], np.array([float(x) for x in clean_line_split]))\n",
        "\n",
        "label_vector_data = raw_data.map(parse_interaction_with_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBKVRQX5-ZvK"
      },
      "source": [
        "The next step is not very sophisticated. We use `filter` on the RDD to leave out other labels but the one we want to gather statistics from.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTQQtNjW-ZvL"
      },
      "outputs": [],
      "source": [
        "normal_label_data = label_vector_data.filter(lambda x: x[0]==\"normal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y002R4fh-ZvL"
      },
      "source": [
        "Now we can use the new RDD to call `colStats` on the values.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btWXec6h-ZvL"
      },
      "outputs": [],
      "source": [
        "normal_summary = Statistics.colStats(normal_label_data.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ru2Y2kh-ZvL"
      },
      "source": [
        "And collect the results as we did before.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZALJB3m-ZvL"
      },
      "outputs": [],
      "source": [
        "print (\"Duration Statistics for label: {}\".format(\"normal\"))\n",
        "print (\" Mean: {}\".format(normal_summary.mean()[0],3))\n",
        "print (\" St. deviation: {}\".format(round(sqrt(normal_summary.variance()[0]),3)))\n",
        "print (\" Max value: {}\".format(round(normal_summary.max()[0],3)))\n",
        "print (\" Min value: {}\".format(round(normal_summary.min()[0],3)))\n",
        "print (\" Total value count: {}\".format(normal_summary.count()))\n",
        "print (\" Number of non-zero values: {}\".format(normal_summary.numNonzeros()[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcfwv5Jc-ZvL"
      },
      "source": [
        "Instead of working with a key/value pair we could have just filter our raw data split using the label in column 41. Then we can parse the results as we did before. This will work as well. However having our data organised as key/value pairs will open the door to better manipulations. Since `values()` is a transformation on an RDD, and not an action, we don't perform any computation until we call `colStats` anyway.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k97fBwGM-ZvM"
      },
      "source": [
        "But lets wrap this within a function so we can reuse it with any label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stEpaH-G-ZvM"
      },
      "outputs": [],
      "source": [
        "def summary_by_label(raw_data, label):\n",
        "    label_vector_data = raw_data.map(parse_interaction_with_key).filter(lambda x: x[0]==label)\n",
        "    return Statistics.colStats(label_vector_data.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCWaf62q-ZvM"
      },
      "source": [
        "Let's give it a try with the \"normal.\" label again.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnJUIV0c-ZvM"
      },
      "outputs": [],
      "source": [
        "normal_sum = summary_by_label(raw_data, \"normal.\")\n",
        "\n",
        "print (\"Duration Statistics for label: {}\".format(\"normal\"))\n",
        "print (\" Mean: {}\".format(normal_sum.mean()[0],3))\n",
        "print (\" St. deviation: {}\".format(round(sqrt(normal_sum.variance()[0]),3)))\n",
        "print (\" Max value: {}\".format(round(normal_sum.max()[0],3)))\n",
        "print (\" Min value: {}\".format(round(normal_sum.min()[0],3)))\n",
        "print (\" Total value count: {}\".format(normal_sum.count()))\n",
        "print (\" Number of non-zero values: {}\".format(normal_sum.numNonzeros()[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QROa2LPM-ZvM"
      },
      "source": [
        "Let's try now with some network attack. We have all of them listed [here](http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "uS-8RexM-ZvM"
      },
      "outputs": [],
      "source": [
        "guess_passwd_summary = summary_by_label(raw_data, \"guess_passwd.\")\n",
        "\n",
        "print (\"Duration Statistics for label: {}\".format(\"guess_password\"))\n",
        "print (\" Mean: {}\".format(guess_passwd_summary.mean()[0],3))\n",
        "print (\" St. deviation: {}\".format(round(sqrt(guess_passwd_summary.variance()[0]),3)))\n",
        "print (\" Max value: {}\".format(round(guess_passwd_summary.max()[0],3)))\n",
        "print (\" Min value: {}\".format(round(guess_passwd_summary.min()[0],3)))\n",
        "print (\" Total value count: {}\".format(guess_passwd_summary.count()))\n",
        "print (\" Number of non-zero values: {}\".format(guess_passwd_summary.numNonzeros()[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYS_1ben-ZvN"
      },
      "source": [
        "We can see that this type of attack is shorter in duration than a normal interaction. We could build a table with duration statistics for each type of interaction in our dataset. First we need to get a list of labels as described in the first line [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names).      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNeWVDaU-ZvN"
      },
      "outputs": [],
      "source": [
        "label_list = [\"back.\",\"buffer_overflow.\",\"ftp_write.\",\"guess_passwd.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elqYVev8-ZvN"
      },
      "source": [
        "### Summary statistics by label  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRw23lAN-ZvN"
      },
      "source": [
        "Then we get a list of statistics for each label.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFL0dkxN-ZvN"
      },
      "outputs": [],
      "source": [
        "stats_by_label = [(label, summary_by_label(raw_data, label)) for label in label_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4aSQa1k-ZvO"
      },
      "source": [
        "Now we get the *duration* column, first in our dataset (i.e. index 0).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8OpOV3X-ZvO"
      },
      "outputs": [],
      "source": [
        "duration_by_index = [stat[0] for stat in stats_by_label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaBvcm5Q-ZvO"
      },
      "outputs": [],
      "source": [
        "duration_by_label = [ \n",
        "    (np.array([float(stat[1].mean()[0]), float(sqrt(stat[1].variance()[0])), float(stat[1].min()[0]), float(stat[1].max()[0]), int(stat[1].count())])) \n",
        "    for stat in stats_by_label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmQllVRj-ZvO"
      },
      "outputs": [],
      "source": [
        "duration_by_label[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preFu04H-ZvO"
      },
      "source": [
        "That we can put into a Pandas data frame.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Ek9852-ZvO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 50)\n",
        "df = pd.DataFrame(duration_by_label, index= duration_by_index)\n",
        "stats_by_label_df=df.rename(columns={0: \"Mean\", 1: \"Std Dev\", 2:\"Min\", 3:\"Max\", 4:\"Count\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcGEz8-I-ZvO"
      },
      "source": [
        "And print it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3667fs9-ZvP"
      },
      "outputs": [],
      "source": [
        "print (\"Duration statistics, by label\")\n",
        "stats_by_label_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3q8Qtfp-ZvP"
      },
      "source": [
        "In order to reuse this code and get a dataframe from any variable in our dataset we will define a function.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBCZxMb3-ZvP"
      },
      "outputs": [],
      "source": [
        "def get_variable_stats_df(stats_by_label, column_i):\n",
        "    \n",
        "    column_stats_by_label_index = [stat[0] for stat in stats_by_label]\n",
        "    column_stats_by_label = [ \n",
        "    (np.array([float(stat[1].mean()[0]), float(sqrt(stat[1].variance()[0])), float(stat[1].min()[0]), float(stat[1].max()[0]), int(stat[1].count())])) \n",
        "    for stat in stats_by_label]\n",
        "    \n",
        "    df = pd.DataFrame(column_stats_by_label, index= column_stats_by_label_index)\n",
        "    df=df.rename(columns={0: \"Mean\", 1: \"Std Dev\", 2:\"Min\", 3:\"Max\", 4:\"Count\"})\n",
        "    return df\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On4fnOMk-ZvQ"
      },
      "source": [
        "Let's try for *duration* again.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI9MqEP9-ZvQ"
      },
      "outputs": [],
      "source": [
        "get_variable_stats_df(stats_by_label,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TyQR0jH-ZvQ"
      },
      "source": [
        "Now for the next numeric column in the dataset, *src_bytes*.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzUyD-vr-ZvQ"
      },
      "outputs": [],
      "source": [
        "print (\"src_bytes statistics, by label\")\n",
        "get_variable_stats_df(stats_by_label,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2BmKsPO-ZvQ"
      },
      "source": [
        "And so on. By reusing the `summary_by_label` and `get_variable_stats_df` functions we can perform some exploratory data analysis in large datasets with Spark.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LAe8Zh-ZvR"
      },
      "source": [
        "## Correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRFEqATJ-ZvR"
      },
      "source": [
        "Spark's MLlib supports [Pearson’s](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) and [Spearman’s](http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) to calculate pairwise correlation methods among many series. Both of them are provided by the `corr` method in the `Statistics` package.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmZmmMBP-ZvR"
      },
      "source": [
        "We have two options as input. Either two `RDD[Double]`s or an `RDD[Vector]`. In the first case the output will be a `Double` value, while in the second a whole correlation Matrix. Due to the nature of our data, we will obtain the second.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDhUh-sf-ZvR"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.stat import Statistics \n",
        "correlation_matrix = Statistics.corr(vector_data, method=\"spearman\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcRVEfRD-ZvR"
      },
      "source": [
        "Once we have the correlations ready, we can start inspecting their values.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3drC1UQz-ZvR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 50)\n",
        "\n",
        "col_names = [\"duration\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\n",
        "             \"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n",
        "             \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n",
        "             \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "             \"is_hot_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "             \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "             \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "             \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "             \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "             \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n",
        "\n",
        "corr_df = pd.DataFrame(correlation_matrix, index=col_names, columns=col_names)\n",
        "\n",
        "corr_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGVPap3I-ZvS"
      },
      "source": [
        "We have used a *Pandas* `DataFrame` here to render the correlation matrix in a more comprehensive way. Now we want those variables that are highly correlated. For that we do a bit of dataframe manipulation.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONZdViXt-ZvS"
      },
      "outputs": [],
      "source": [
        "# get a boolean dataframe where true means that a pair of variables is highly correlated\n",
        "highly_correlated_df = (abs(corr_df) > .8) & (corr_df < 1.0)\n",
        "# get the names of the variables so we can use them to slice the dataframe\n",
        "correlated_vars_index = (highly_correlated_df==True).any()\n",
        "correlated_var_names = correlated_vars_index[correlated_vars_index==True].index\n",
        "# slice it\n",
        "highly_correlated_df.loc[correlated_var_names,correlated_var_names]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTC6NTA-ZvS"
      },
      "source": [
        "### Conclusions and possible model selection hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeR8HEPc-ZvS"
      },
      "source": [
        "The previous dataframe showed us which variables are highly correlated. We have kept just those variables with at least one strong correlation. We can use as we please, but a good way could be to do some model selection. That is, if we have a group of variables that are highly correlated, we can keep just one of them to represent the group under the assumption that they convey similar information as predictors. Reducing the number of variables will not improve our model accuracy, but it will make it easier to understand and also more efficient to compute.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M7unQoQ-ZvS"
      },
      "source": [
        "For example, from the description of the [KDD Cup 99 task](http://kdd.ics.uci.edu/databases/kddcup99/task.html) we know that the variable `dst_host_same_src_port_rate` references the percentage of the last 100 connections to the same port, for the same destination host. In our correlation matrix (and auxiliar dataframes) we find that this one is highly and positively correlated to `src_bytes` and `srv_count`. The former is the number of bytes sent form source to destination. The later is the number of connections to the same service as the current connection in the past 2 seconds. We might decide not to include `dst_host_same_src_port_rate` in our model if we include the other two, as a way to reduce the number of variables and later one better interpret our models.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PHzhQX_-ZvS"
      },
      "source": [
        "Later on, in those notebooks dedicated to build predictive models, we will make use of this information to build more interpretable models.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIeO4UNd-oUg"
      },
      "source": [
        "# MLlib: Classification with Logistic Regression - Walkthrough Example 7 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bueviYEV-oUh"
      },
      "source": [
        "In this notebook we will use Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) to build a **Logistic Regression** classifier for network attack detection. We will use the complete [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) datasets in order to test Spark capabilities with large datasets.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3k9ijo3-oUh"
      },
      "source": [
        "Additionally, we will introduce two ways of performing **model selection**: by using a correlation matrix and by using hypothesis testing.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVcDCgrv-oUi"
      },
      "source": [
        "At the time of processing this notebook, our Spark cluster contains:  \n",
        "\n",
        "- Eight nodes, with one of them acting as master and the rest as workers.  \n",
        "- Each node contains 8Gb of RAM, with 6Gb being used for each node.  \n",
        "- Each node has a 2.4Ghz Intel dual core processor.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBV58ke4-oUi"
      },
      "source": [
        "## Getting the data and creating the RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4ZSRWzF-oUi"
      },
      "source": [
        "As we said, this time we will use the complete dataset provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million network interactions. The file is provided as a Gzip file that we will download locally.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDHhZT3W-oUj"
      },
      "outputs": [],
      "source": [
        "data_file = data_path+ \"kddcup.data.gz\"\n",
        "raw_data = sc.textFile(data_file)\n",
        "\n",
        "print(\"Train data size is {}\".format(raw_data.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CMRtGVy-oUk"
      },
      "source": [
        "The [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) also provide test data that we will load in a separate RDD.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol8jv-dK-oUk"
      },
      "outputs": [],
      "source": [
        "test_data_file = data_path+\"corrected.gz\"\n",
        "test_raw_data = sc.textFile(test_data_file)\n",
        "#test_raw_data.count()\n",
        "print(\"Test data size is {}\".format(test_raw_data.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7MlaUxN-oUl"
      },
      "source": [
        "## Labeled Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTSdLsIm-oUl"
      },
      "source": [
        "A labeled point is a local vector associated with a label/response. In [MLlib](https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point), labeled points are used in supervised learning algorithms and they are stored as doubles. For binary classification, a label should be either 0 (negative) or 1 (positive).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihNrMpvn-oUl"
      },
      "source": [
        "### Preparing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYtajgJ2-oUm"
      },
      "source": [
        "In our case, we are interested in detecting network attacks in general. We don't need to detect which type of attack we are dealing with. Therefore we will tag each network interaction as non attack (i.e. 'normal' tag) or attack (i.e. anything else but 'normal').  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6NUZFo7-oUm"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from numpy import array\n",
        "\n",
        "def parse_interaction(line):\n",
        "    line_split = line.split(\",\")\n",
        "    # leave_out = [1,2,3,41]\n",
        "    clean_line_split = line_split[0:1]+line_split[4:41]\n",
        "    attack = 1.0\n",
        "    if line_split[41]=='normal.':\n",
        "        attack = 0.0\n",
        "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
        "\n",
        "training_data = raw_data.map(parse_interaction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqLFtic1-oUm"
      },
      "source": [
        "### Preparing the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVyawd1V-oUm"
      },
      "source": [
        "Similarly, we process our test data file.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDgV8bFv-oUm"
      },
      "outputs": [],
      "source": [
        "test_data = test_raw_data.map(parse_interaction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvSH2I1w-oUm"
      },
      "source": [
        "## Detecting network attacks using Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eTuhXsI-oUm"
      },
      "source": [
        "[Logistic regression](http://en.wikipedia.org/wiki/Logistic_regression) is widely used to predict a binary response. Spark implements [two algorithms](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression) to solve logistic regression: mini-batch gradient descent and L-BFGS. L-BFGS is recommended over mini-batch gradient descent for faster convergence.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DnVkITZ-oUm"
      },
      "source": [
        "### Training a classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRPsCfiX-oUn"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "from time import time\n",
        "\n",
        "# Build the model\n",
        "t0 = time()\n",
        "logit_model = LogisticRegressionWithLBFGS.train(training_data)\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylyJZFO6-oUn"
      },
      "source": [
        "### Evaluating the model on new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30Or4y_I-oUn"
      },
      "source": [
        "In order to measure the classification error on our test data, we use `map` on the `test_data` RDD and the model to predict each test point class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LAzIMWw-oUo"
      },
      "outputs": [],
      "source": [
        "labels_and_preds = test_data.map(lambda p: (p.label, logit_model.predict(p.features)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GoTEO0W-oUo"
      },
      "source": [
        "Classification results are returned in pars, with the actual test label and the predicted one. This is used to calculate the classification error by using `filter` and `count` as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHjJtL2Z-oUo"
      },
      "outputs": [],
      "source": [
        "labels_and_preds.take(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0nNLjAH-oUo"
      },
      "outputs": [],
      "source": [
        "t0 = time()\n",
        "test_accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI-KobI3-oUo"
      },
      "source": [
        "That's a decent accuracy. We know that there is space for improvement with a better variable selection and also by including categorical variables (e.g. we have excluded 'protocol' and 'service')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tPn64sE-oUp"
      },
      "source": [
        "## Model selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCHiZD3f-oUp"
      },
      "source": [
        "Model or feature selection helps us building more interpretable and efficient models (or a classifier in this case). For illustrative purposes, we will follow two different approaches, correlation matrices and hypothesis testing.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzh8p-8H-oUp"
      },
      "source": [
        "### Using a correlation matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BysGwK-O-oUp"
      },
      "source": [
        "In a [previous notebook](https://github.com/jadianes/spark-py-notebooks/blob/master/nb7-mllib-statistics/nb7-mllib-statistics.ipynb) we calculated a correlation matrix in order to find predictors that are highly correlated. There are many possible choices there in order to simplify our model. We can pick different combinations of correlated variables and leave just those that represent them. The reader can try different combinations. Here we will choose the following for illustrative purposes:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h33S28KR-oUp"
      },
      "source": [
        "- From the description of the [KDD Cup 99 task](http://kdd.ics.uci.edu/databases/kddcup99/task.html) we know that the variable `dst_host_same_src_port_rate` references the percentage of the last 100 connections to the same port, for the same destination host. In our correlation matrix (and auxiliary dataframes) we find that this one is highly and positively correlated to `src_bytes` and `srv_count`. The former is the number of bytes sent form source to destination. The later is the number of connections to the same service as the current connection in the past 2 seconds. We decide not to include `dst_host_same_src_port_rate` in our model since we include the other two.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QldZBXhg-oUp"
      },
      "source": [
        "- Variables `serror_rate` and `srv_error_rate` (% of connections that have *SYN* errors for same host and same service respectively) are highly positively correlated. Moreover, the set of variables that they highly correlate with are pretty much the same. They look like contributing very similarly to our model. We will keep just `serror_rate`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj9yk8Fm-oUp"
      },
      "source": [
        "- A similar situation happens with `rerror_rate` and `srv_rerror_rate` (% of connections that have *REJ* errors) so we will keep just `rerror_rate`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDvv1uNv-oUq"
      },
      "source": [
        "- Same thing with variables prefixed with `dst_host_` for the previous ones (e.g. `dst_host_srv_serror_rate`).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ko196WP-oUq"
      },
      "source": [
        "We will stop here, although the reader can keep experimenting removing correlated variables has before (e.g. `same_srv_rate` and `diff_srv_rate` are good candidates. Our list of variables we will drop includes:  \n",
        "- `dst_host_same_src_port_rate`, (column 35).  \n",
        "- `srv_serror_rate` (column 25).  \n",
        "- `srv_rerror_rate` (column 27).  \n",
        "- `dst_host_srv_serror_rate` (column 38).  \n",
        "- `dst_host_srv_rerror_rate` (column 40).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnPuqrJV-oUq"
      },
      "source": [
        "#### Evaluating the new model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C79GXiMx-oUq"
      },
      "source": [
        "Let's proceed with the evaluation of our reduced model. First we need to provide training and testing datasets containing just the selected variables. For that we will define a new function to parse the raw data that keeps just what we need.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prap474v-oUq"
      },
      "outputs": [],
      "source": [
        "def parse_interaction_corr(line):\n",
        "    line_split = line.split(\",\")\n",
        "    # leave_out = [1,2,3,25,27,35,38,40,41]\n",
        "    clean_line_split = line_split[0:1]+line_split[4:25]+line_split[26:27]+line_split[28:35]+line_split[36:38]+line_split[39:40]\n",
        "    attack = 1.0\n",
        "    if line_split[41]=='normal.':\n",
        "        attack = 0.0\n",
        "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
        "\n",
        "corr_reduced_training_data = raw_data.map(parse_interaction_corr)\n",
        "corr_reduced_test_data = test_raw_data.map(parse_interaction_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTs1fDqp-oUq"
      },
      "source": [
        "*Note: when selecting elements in the split, a list comprehension with a `leave_out` list for filtering is more Pythonic than slicing and concatenation indeed, but we have found it less efficient. This is very important when dealing with large datasets. The `parse_interaction` functions will be called for every element in the RDD, so we need to make them as efficient as possible.*  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQsNCkXk-oUq"
      },
      "source": [
        "Now we can train the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL8cXwEI-oUq"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "t0 = time()\n",
        "logit_model_2 = LogisticRegressionWithLBFGS.train(corr_reduced_training_data)\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzxGXTGy-oUr"
      },
      "source": [
        "And evaluate its accuracy on the test data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUe_yR2u-oUr"
      },
      "outputs": [],
      "source": [
        "labels_and_preds = corr_reduced_test_data.map(lambda p: (p.label, logit_model_2.predict(p.features)))\n",
        "t0 = time()\n",
        "test_accuracy = labels_and_preds.filter(lambda v: v[0] == v[1]).count() / float(corr_reduced_test_data.count())\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8NRp9mB-oUr"
      },
      "source": [
        "As expected, we have reduced accuracy and also training time. However this doesn't seem a good trade! At least not for logistic regression and considering the predictors we decided to leave out. We have lost quite a lot of accuracy and have not gained a lot of execution time during training. Moreover prediction time didn't improve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLS5W71z-oUr"
      },
      "source": [
        "### Using hypothesis testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7teyvE_G-oUr"
      },
      "source": [
        "Hypothesis testing is a powerful tool in statistical inference and learning to determine whether a result is statistically significant. MLlib supports Pearson's chi-squared (χ2) tests for goodness of fit and independence. The goodness of fit test requires an input type of `Vector`, whereas the independence test requires a `Matrix` as input. Moreover, MLlib also supports the input type `RDD[LabeledPoint]` to enable feature selection via chi-squared independence tests. Again, these methods are part of the [`Statistics`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics) package.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wePhtDu-oUs"
      },
      "source": [
        "In our case we want to perform some sort of feature selection, so we will provide an RDD of `LabeledPoint`. Internally, MLlib will calculate a contingency matrix and perform the Persons's chi-squared (χ2) test. Features need to be categorical. Real-valued features will be treated as categorical in each of its different values. There is a limit of 1000 different values, so we need either to leave out some features or categorise them. In this case, we will consider just features that either take boolean values or just a few different numeric values in our dataset. We could overcome this limitation by defining a more complex `parse_interaction` function that categorises each feature properly.       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STEzWTAg-oUs"
      },
      "outputs": [],
      "source": [
        "feature_names = [\"land\",\"wrong_fragment\",\n",
        "             \"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n",
        "             \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n",
        "             \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "             \"is_hot_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "             \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "             \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "             \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "             \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "             \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQLMN10x-oUs"
      },
      "outputs": [],
      "source": [
        "def parse_interaction_categorical(line):\n",
        "    line_split = line.split(\",\")\n",
        "    clean_line_split = line_split[6:41]\n",
        "    attack = 1.0\n",
        "    if line_split[41]=='normal.':\n",
        "        attack = 0.0\n",
        "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
        "\n",
        "training_data_categorical = raw_data.map(parse_interaction_categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwGoOOfG-oUs"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.stat import Statistics\n",
        "\n",
        "chi = Statistics.chiSqTest(training_data_categorical)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X88s8aJ-oUs"
      },
      "source": [
        "Now we can check the resulting values after putting them into a *Pandas* data frame.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MrG55FL-oUs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 30)\n",
        "\n",
        "records = [(result.statistic, result.pValue) for result in chi]\n",
        "\n",
        "chi_df = pd.DataFrame(data=records, index= feature_names, columns=[\"Statistic\",\"p-value\"])\n",
        "\n",
        "chi_df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCoqPu2f-oUt"
      },
      "source": [
        "From that we conclude that predictors `land` and `num_outbound_cmds` could be removed from our model without affecting our accuracy dramatically. Let's try this.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYFeeFKn-oUt"
      },
      "source": [
        "#### Evaluating the new model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy68hldt-oUt"
      },
      "source": [
        "So the only modification to our first `parse_interaction` function will be to remove columns 6 and 19, corresponding to the two predictors that we want not to be part of our model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y37xSH5e-oUt"
      },
      "outputs": [],
      "source": [
        "def parse_interaction_chi(line):\n",
        "    line_split = line.split(\",\")\n",
        "    # leave_out = [1,2,3,6,19,41]\n",
        "    clean_line_split = line_split[0:1] + line_split[4:6] + line_split[7:19] + line_split[20:41]\n",
        "    attack = 1.0\n",
        "    if line_split[41]=='normal.':\n",
        "        attack = 0.0\n",
        "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
        "\n",
        "training_data_chi = raw_data.map(parse_interaction_chi)\n",
        "test_data_chi = test_raw_data.map(parse_interaction_chi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLpIgw2m-oUt"
      },
      "source": [
        "Now we build the logistic regression classifier again.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRHNzdtA-oUt"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "t0 = time()\n",
        "logit_model_chi = LogisticRegressionWithLBFGS.train(training_data_chi)\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGCKNOH_-oUt"
      },
      "source": [
        "And evaluate in test data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTdxOoOI-oUt"
      },
      "outputs": [],
      "source": [
        "labels_and_preds = test_data_chi.map(lambda p: (p.label, logit_model_chi.predict(p.features)))\n",
        "t0 = time()\n",
        "test_accuracy = labels_and_preds.filter(lambda v: v[0] == p[1]).count() / float(test_data_chi.count())\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYHj7EZF-oUu"
      },
      "source": [
        "So we can see that, by using hypothesis testing, we have been able to remove two predictors without diminishing testing accuracy at all. Training time improved a bit as well. This might now seem like a big model reduction, but it is something when dealing with big data sources. Moreover, we should be able to categorise those five predictors we have left out for different reasons and, either include them in the model or leave them out if they aren't statistically significant.  \n",
        "\n",
        "Additionally, we could try to remove some of those predictors that are highly correlated, trying not to reduce accuracy too much. In the end, we should end up with a model easier to understand and use.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc8GmcSC-zRh"
      },
      "source": [
        "# MLlib: Decision Trees  - Walkthrough Example 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXb-ycFX-zRj"
      },
      "source": [
        "In this notebook we will use Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) to build a **Decision Tree** classifier for network attack detection. We will use the complete [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) datasets in order to test Spark capabilities with large datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoLj-86h-zRk"
      },
      "source": [
        "Decision trees are a popular machine learning tool in part because they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions. In this notebook, we will first train a classification tree including every single predictor. Then we will use our results to perform model selection. Once we find out the most important ones (the main splits in the tree) we will build a minimal tree using just three of them (the first two levels of the tree in order to compare performance and accuracy.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Le-hS-r-zRk"
      },
      "source": [
        "At the time of processing this notebook, our Spark cluster contains:  \n",
        "\n",
        "- Eight nodes, with one of them acting as master and the rest as workers.  \n",
        "- Each node contains 8Gb of RAM, with 6Gb being used for each node.  \n",
        "- Each node has a 2.4Ghz Intel dual core processor.  \n",
        "- Running Apache Spark 1.3.1.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1csWHYg-zRk"
      },
      "source": [
        "## Getting the data and creating the RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTqhelIs-zRl"
      },
      "outputs": [],
      "source": [
        "data_file = data_path+\"kddcup.data.gz\"\n",
        "raw_data = sc.textFile(data_file)\n",
        "\n",
        "print (\"Train data size is {}\".format(raw_data.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mxie_Na-zRl"
      },
      "source": [
        "The [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) also provide test data that we will load in a separate RDD.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n-ajSss-zRl"
      },
      "outputs": [],
      "source": [
        "test_data_file = data_path+\"corrected.gz\"\n",
        "test_raw_data = sc.textFile(test_data_file)\n",
        "\n",
        "print (\"Test data size is {}\".format(test_raw_data.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH0oYCuS-zRm"
      },
      "source": [
        "## Detecting network attacks using Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L-TFygf-zRm"
      },
      "source": [
        "In this section we will train a *classification tree* that, as we did with *logistic regression*, will predict if a network interaction is either `normal` or `attack`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkZ4dcUa-zRm"
      },
      "source": [
        "Training a classification tree using [MLlib](https://spark.apache.org/docs/latest/mllib-decision-tree.html) requires some parameters:  \n",
        "- Training data  \n",
        "- Num classes  \n",
        "- Categorical features info: a map from column to categorical variables arity. This is optional, although it should increase model accuracy. However it requires that we know the levels in our categorical variables in advance. second we need to parse our data to convert labels to integer values within the arity range.  \n",
        "- Impurity metric  \n",
        "- Tree maximum depth  \n",
        "- And tree maximum number of bins  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFWacwHD-zRm"
      },
      "source": [
        "In the next section we will see how to obtain all the labels within a dataset and convert them to numerical factors.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umIro0C5-zRn"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMMya8bt-zRo"
      },
      "source": [
        "As we said, in order to benefits from trees ability to seamlessly with categorical variables, we need to convert them to numerical factors. But first we need to obtain all the possible levels. We will use *set* transformations on a csv parsed RDD.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT63vKE3-zRo"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from numpy import array\n",
        "\n",
        "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
        "test_csv_data = test_raw_data.map(lambda x: x.split(\",\"))\n",
        "\n",
        "protocols = csv_data.map(lambda x: x[1]).distinct().collect()\n",
        "services = csv_data.map(lambda x: x[2]).distinct().collect()\n",
        "flags = csv_data.map(lambda x: x[3]).distinct().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaV_Qwq3-zRo"
      },
      "source": [
        "And now we can use this Python lists in our `create_labeled_point` function. If a factor level is not in the training data, we assign an especial level. Remember that we cannot use testing data for training our model, not even the factor levels. The testing data represents the unknown to us in a real case.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTh8YgpV-zRp"
      },
      "outputs": [],
      "source": [
        "def create_labeled_point(line_split):\n",
        "    # leave_out = [41]\n",
        "    clean_line_split = line_split[0:41]\n",
        "    \n",
        "    # convert protocol to numeric categorical variable\n",
        "    try: \n",
        "        clean_line_split[1] = protocols.index(clean_line_split[1])\n",
        "    except:\n",
        "        clean_line_split[1] = len(protocols)\n",
        "        \n",
        "    # convert service to numeric categorical variable\n",
        "    try:\n",
        "        clean_line_split[2] = services.index(clean_line_split[2])\n",
        "    except:\n",
        "        clean_line_split[2] = len(services)\n",
        "    \n",
        "    # convert flag to numeric categorical variable\n",
        "    try:\n",
        "        clean_line_split[3] = flags.index(clean_line_split[3])\n",
        "    except:\n",
        "        clean_line_split[3] = len(flags)\n",
        "    \n",
        "    # convert label to binary label\n",
        "    attack = 1.0\n",
        "    if line_split[41]=='normal.':\n",
        "        attack = 0.0\n",
        "        \n",
        "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
        "\n",
        "training_data = csv_data.map(create_labeled_point)\n",
        "test_data = test_csv_data.map(create_labeled_point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4f4Ovb-zRp"
      },
      "source": [
        "### Training a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypSA1dvL-zRp"
      },
      "source": [
        "We are now ready to train our classification tree. We will keep the `maxDepth` value small. This will lead to smaller accuracy, but we will obtain less splits so later on we can better interpret the tree. In a production system we will try to increase this value in order to find a better accuracy.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb2ttqFL-zRp"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
        "from time import time\n",
        "\n",
        "# Build the model\n",
        "t0 = time()\n",
        "tree_model = DecisionTree.trainClassifier(training_data, numClasses=2, \n",
        "                                          categoricalFeaturesInfo={1: len(protocols), 2: len(services), 3: len(flags)},\n",
        "                                          impurity='gini', maxDepth=4, maxBins=100)\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsmD5OK2-zRq"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFl-_J6j-zRq"
      },
      "source": [
        "In order to measure the classification error on our test data, we use `map` on the `test_data` RDD and the model to predict each test point class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpFsLGHq-zRq"
      },
      "outputs": [],
      "source": [
        "predictions = tree_model.predict(test_data.map(lambda p: p.features))\n",
        "labels_and_preds = test_data.map(lambda p: p.label).zip(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZIIvzYm-zRq"
      },
      "source": [
        "Classification results are returned in pars, with the actual test label and the predicted one. This is used to calculate the classification error by using `filter` and `count` as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZzwbiJd-zRq"
      },
      "outputs": [],
      "source": [
        "t0 = time()\n",
        "test_accuracy = labels_and_preds.filter(lambda v: v[0] == v[1]).count() / float(test_data.count())\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwrJzr7n-zRr"
      },
      "source": [
        "*NOTE: the zip transformation doesn't work properly with pySpark 1.2.1. It does in 1.3*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFBw8V7x-zRr"
      },
      "source": [
        "### Interpreting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWymi_LP-zRr"
      },
      "source": [
        "Understanding our tree splits is a great exercise in order to explain our classification labels in terms of predictors and the values they take. Using the `toDebugString` method in our three model we can obtain a lot of information regarding splits, nodes, etc.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x7WogaC-zRr"
      },
      "outputs": [],
      "source": [
        "print (\"Learned classification tree model:\")\n",
        "print (tree_model.toDebugString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWaACqPd-zRr"
      },
      "source": [
        "For example, a network interaction with the following features (see description [here](http://kdd.ics.uci.edu/databases/kddcup99/task.html)) will be classified as an attack by our model:  \n",
        "- `count`, the number of connections to the same host as the current connection in the past two seconds, being greater than 32. \n",
        "- `dst_bytes`, the number of data bytes from destination to source, is 0.  \n",
        "- `service` is neither level 0 nor 52.  \n",
        "- `logged_in` is false.  \n",
        "From our services list we know that:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22PzS97X-zRs"
      },
      "outputs": [],
      "source": [
        "print (\"Service 0 is {}\".format(services[0]))\n",
        "print (\"Service 52 is {}\".format(services[52]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukqpES2z-zRs"
      },
      "source": [
        "So we can characterise network interactions with more than 32 connections to the same server in the last 2 seconds, transferring zero bytes from destination to source, where service is neither *urp_i* nor *tftp_u*, and not logged in, as network attacks. A similar approach can be used for each tree terminal node.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0bUfwX8-zRs"
      },
      "source": [
        "We can see that `count` is the first node split in the tree. Remember that each partition is chosen greedily by selecting the best split from a set of possible splits, in order to maximize the information gain at a tree node (see more [here](https://spark.apache.org/docs/latest/mllib-decision-tree.html#basic-algorithm)). At a second level we find variables `flag` (normal or error status of the connection) and `dst_bytes` (the number of data bytes from destination to source) and so on.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5oQ5XlL-zRs"
      },
      "source": [
        "This explaining capability of a classification (or regression) tree is one of its main benefits. Understaining data is a key factor to build better models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM1SXRyQ-zRs"
      },
      "source": [
        "## Building a minimal model using the three main splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPLt4eGP_Wsg"
      },
      "source": [
        "# Spark SQL and Data Frames - Walkthrough Example 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCqy1wIC_Wsi"
      },
      "source": [
        "This notebook will introduce Spark capabilities to deal with data in a structured way. Basically, everything turns around the concept of *Data Frame* and using *SQL language* to query them. We will see how the data frame abstraction, very popular in other data analytics ecosystems (e.g. R and Python/Pandas), it is very powerful when performing exploratory data analysis. In fact, it is very easy to express data queries when used together with the SQL language. Moreover, Spark distributes this column-based data structure transparently, in order to make the querying process as efficient as possible.      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRSk2CkJ_Wsi"
      },
      "source": [
        "## Getting the data and creating the RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vax8wqCL_Wsi"
      },
      "outputs": [],
      "source": [
        "data_file = data_path+\"kddcup.data_10_percent.gz\"\n",
        "raw_data = sc.textFile(data_file).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyXIz0gz_Wsj"
      },
      "source": [
        "## Getting a Data Frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY1bAZKQ_Wsj"
      },
      "source": [
        "A Spark `DataFrame` is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R or Pandas. They can be constructed from a wide array of sources such as a existing RDD in our case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4d4yXAl_Wsk"
      },
      "source": [
        "The entry point into all SQL functionality in Spark is the `SQLContext` class. To create a basic instance, all we need is a `SparkContext` reference. Since we are running Spark in shell mode (using pySpark) we can use the global context object `sc` for this purpose.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvsF5vqw_Wsk"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QkO5RBH_Wsl"
      },
      "source": [
        "### Inferring the schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wt-GCsn_Wsl"
      },
      "source": [
        "With a `SQLContext`, we are ready to create a `DataFrame` from our existing RDD. But first we need to tell Spark SQL the schema in our data.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRfnRGBE_Wsl"
      },
      "source": [
        "Spark SQL can convert an RDD of `Row` objects to a `DataFrame`. Rows are constructed by passing a list of key/value pairs as *kwargs* to the `Row` class. The keys define the column names, and the types are inferred by looking at the first row. Therefore, it is important that there is no missing data in the first row of the RDD in order to properly infer the schema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY35ZsOI_Wsl"
      },
      "source": [
        "In our case, we first need to split the comma separated data, and then use the information in KDD's 1999 task description to obtain the [column names](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aeeNT_S_Wsm"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
        "row_data = csv_data.map(lambda p: Row(\n",
        "    duration=int(p[0]), \n",
        "    protocol_type=p[1],\n",
        "    service=p[2],\n",
        "    flag=p[3],\n",
        "    src_bytes=int(p[4]),\n",
        "    dst_bytes=int(p[5])\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SqC2Ghv_Wsm"
      },
      "source": [
        "Once we have our RDD of `Row` we can infer and register the schema.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_vXwn6S_Wsm"
      },
      "outputs": [],
      "source": [
        "interactions_df = sqlContext.createDataFrame(row_data)\n",
        "interactions_df.registerTempTable(\"interactions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hehIgx25_Wsn"
      },
      "source": [
        "Now we can run SQL queries over our data frame that has been registered as a table.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt4prOE8_Wsn"
      },
      "outputs": [],
      "source": [
        "# Select tcp network interactions with more than 1 second duration and no transfer from destination\n",
        "tcp_interactions = sqlContext.sql(\"\"\"\n",
        "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
        "\"\"\")\n",
        "tcp_interactions.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7jGzFL9_Wsn"
      },
      "source": [
        "The results of SQL queries are RDDs and support all the normal RDD operations.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_kkEUqI_Wsn"
      },
      "outputs": [],
      "source": [
        "# Output duration together with dst_bytes\n",
        "tcp_interactions_out = tcp_interactions.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))\n",
        "for ti_out in tcp_interactions_out.collect():\n",
        "    print(ti_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAe4hJhW_Wso"
      },
      "source": [
        "We can easily have a look at our data frame schema using `printSchema`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r19iqw3c_Wso"
      },
      "outputs": [],
      "source": [
        "interactions_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY1BXCo4_Wso"
      },
      "source": [
        "## Queries as `DataFrame` operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PJtvJ0b_Wso"
      },
      "source": [
        "Spark `DataFrame` provides a domain-specific language for structured data manipulation. This language includes methods we can concatenate in order to do selection, filtering, grouping, etc. For example, let's say we want to count how many interactions are there for each protocol type. We can proceed as follows.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dzDni5T_Wso"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "t0 = time()\n",
        "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").groupBy(\"protocol_type\").count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZulheb3_Wsq"
      },
      "source": [
        "Now imagine that we want to count how many interactions last more than 1 second, with no data transfer from destination, grouped by protocol type. We can just add to filter calls to the previous.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgIwj0vf_Wsq"
      },
      "outputs": [],
      "source": [
        "t0 = time()\n",
        "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").filter(interactions_df.duration>1000).filter(interactions_df.dst_bytes==0).groupBy(\"protocol_type\").count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeNSpbcl_Wsq"
      },
      "source": [
        "We can use this to perform some [exploratory data analysis](http://en.wikipedia.org/wiki/Exploratory_data_analysis). Let's count how many attack and normal interactions we have. First we need to add the label column to our data.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvylgGCG_Wsq"
      },
      "outputs": [],
      "source": [
        "def get_label_type(label):\n",
        "    if label!=\"normal.\":\n",
        "        return \"attack\"\n",
        "    else:\n",
        "        return \"normal\"\n",
        "    \n",
        "row_labeled_data = csv_data.map(lambda p: Row(\n",
        "    duration=int(p[0]), \n",
        "    protocol_type=p[1],\n",
        "    service=p[2],\n",
        "    flag=p[3],\n",
        "    src_bytes=int(p[4]),\n",
        "    dst_bytes=int(p[5]),\n",
        "    label=get_label_type(p[41])\n",
        "    )\n",
        ")\n",
        "interactions_labeled_df = sqlContext.createDataFrame(row_labeled_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR9kvonF_Wsq"
      },
      "source": [
        "This time we don't need to register the schema since we are going to use the OO query interface.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFrWzM1q_Wsq"
      },
      "source": [
        "Let's check the previous actually works by counting attack and normal data in our data frame.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c1sbVrN_Wsr"
      },
      "outputs": [],
      "source": [
        "t0 = time()\n",
        "interactions_labeled_df.select(\"label\").groupBy(\"label\").count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-urWgTkF_Wsr"
      },
      "source": [
        "Now we want to count them by label and protocol type, in order to see how important the protocol type is to detect when an interaction is or not an attack.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGrTXYld_Wsr"
      },
      "outputs": [],
      "source": [
        "t0 = time()\n",
        "interactions_labeled_df.select(\"label\", \"protocol_type\").groupBy(\"label\", \"protocol_type\").count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF91w3Vc_Wsr"
      },
      "source": [
        "At first sight it seems that *udp* interactions are in lower proportion between network attacks versus other protocol types.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFCL56gf_Wsr"
      },
      "source": [
        "And we can do much more sophisticated groupings. For example, add to the previous a \"split\" based on data transfer from target.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU6afnnn_Wsr"
      },
      "outputs": [],
      "source": [
        "t0 = time()\n",
        "interactions_labeled_df.select(\"label\", \"protocol_type\", \"dst_bytes\").groupBy(\"label\", \"protocol_type\", interactions_labeled_df.dst_bytes==0).count().show()\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Query performed in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5pjtZPV_Wsr"
      },
      "source": [
        "We see how relevant is this new split to determine if a network interaction is an attack.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeXvY5tW_Wsr"
      },
      "source": [
        "We will stop here, but we can see how powerful this type of queries are in order to explore our data. Actually we can replicate all the splits we saw in previous notebooks, when introducing classification trees, just by selecting, groping, and filtering our dataframe. For a more detailed (but less real-world) list of Spark's `DataFrame` operations and data sources, have a look at the official documentation [here](https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-operations).    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z48f9k_y-zRt"
      },
      "source": [
        "So now that we know the main features predicting a network attack, thanks to our classification tree splits, let's use them to build a minimal classification tree with just the main three variables: `count`, `dst_bytes`, and `flag`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCCbgqV2-zRt"
      },
      "source": [
        "We need to define the appropriate function to create labeled points.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7KIhhpK-zRt"
      },
      "outputs": [],
      "source": [
        "def create_labeled_point_minimal(line_split):\n",
        "    # leave_out = [41]\n",
        "    clean_line_split = line_split[3:4] + line_split[5:6] + line_split[22:23]\n",
        "    \n",
        "    # convert flag to numeric categorical variable\n",
        "    try:\n",
        "        clean_line_split[0] = flags.index(clean_line_split[0])\n",
        "    except:\n",
        "        clean_line_split[0] = len(flags)\n",
        "    \n",
        "    # convert label to binary label\n",
        "    attack = 1.0\n",
        "    if line_split[41]=='normal.':\n",
        "        attack = 0.0\n",
        "        \n",
        "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
        "\n",
        "training_data_minimal = csv_data.map(create_labeled_point_minimal)\n",
        "test_data_minimal = test_csv_data.map(create_labeled_point_minimal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBWZ4o9o-zRt"
      },
      "source": [
        "That we use to train the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTvIy-tC-zRt"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "t0 = time()\n",
        "tree_model_minimal = DecisionTree.trainClassifier(training_data_minimal, numClasses=2, \n",
        "                                          categoricalFeaturesInfo={0: len(flags)},\n",
        "                                          impurity='gini', maxDepth=3, maxBins=32)\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8siYdR8-zRt"
      },
      "source": [
        "Now we can predict on the testing data and calculate accuracy.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy139rTm-zRt"
      },
      "outputs": [],
      "source": [
        "predictions_minimal = tree_model_minimal.predict(test_data_minimal.map(lambda p: p.features))\n",
        "labels_and_preds_minimal = test_data_minimal.map(lambda p: p.label).zip(predictions_minimal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znGiFOyg-zRt"
      },
      "outputs": [],
      "source": [
        "t0 = time()\n",
        "test_accuracy = labels_and_preds_minimal.filter(lambda v: v[0] == v[1]).count() / float(test_data_minimal.count())\n",
        "tt = time() - t0\n",
        "\n",
        "print (\"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT7Nvsgw-zRu"
      },
      "source": [
        "So we have trained a classification tree with just the three most important predictors, in half of the time, and with a not so bad accuracy. In fact, a classification tree is a very good model selection tool!    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "WT_Examples.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}