{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXM2E63BHJms"
      },
      "source": [
        "# Connecting Google Drive to Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxTklRgJRuOR"
      },
      "source": [
        "Notebook created by Rosa Filgueira (https://github.com/rosafilgueira/Seminar_MUIA), materials adapted by Daniel Garijo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSbb-qfwW0G3",
        "outputId": "f7dcfc18-e492-4f78-d8ec-ab1a384fdd62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCi91jDVFybZ"
      },
      "source": [
        "# Installing Apache Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt2zOx2WfRo0",
        "outputId": "c835fab5-f9ed-46d3-d3fc-95bad4ebb012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=21f42b161b7732134d1289a69006ccd88c4272a63230afca582c743737416a9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtpWkOP8ghmM",
        "outputId": "ab5dd3b5-05f6-40e9-dbcb-5cbe9857c7d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  spark-3.3.2-bin-hadoop3  spark-3.3.2-bin-hadoop3.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag0Hrn8IRuOb",
        "outputId": "1b7a4de5-c414-49ab-8b72-c7afa0c70824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.18\n",
            "Branch HEAD\n",
            "Compiled by user liangchi on 2023-02-10T19:57:40Z\n",
            "Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ],
      "source": [
        "!pyspark --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0MVlmq19gMcT",
        "outputId": "67062a7e-faa6-4953-825f-bc7f141bc774"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.3.2-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55fnH6fwFzgQ"
      },
      "source": [
        "# Creating an Apache Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qOo9iYya_X_t"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "c80x16d_BnwI",
        "outputId": "17282637-685e-48ae-bac6-603ddcc2e54f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f434c2a3a60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://08ad74c2ac0d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Learning_Spark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYQk8K1_CW7F",
        "outputId": "bc6d3731-beb0-4dd1-e3be-5062a902e51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  spark-3.3.2-bin-hadoop3  spark-3.3.2-bin-hadoop3.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVqMudWkGGRk"
      },
      "source": [
        "# Creating my first RDDs  (following the exercise in class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDxgk7PdRuOl"
      },
      "source": [
        "A simple way to create an RDD is to take an existing collection and load it into Spark by using the SparkContext's parallelize() method. We first start by creating a list of integers using Python's xrange() method. Following this, we create our first RDD by using the parallelize() method to load the list of numbers unto 8 partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hGYurxu3RuOm"
      },
      "outputs": [],
      "source": [
        "# Create a list of one hundred integers\n",
        "numbers = range(1, 101)\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Create an RDD by dividing the list unto 8 partitions\n",
        "numbersRDD = sc.parallelize(numbers, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5rgXf7rRuOn"
      },
      "source": [
        "Each RDD has a unique identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqdgQnmARuOo",
        "outputId": "5bf5561f-a549-4ea3-f0ef-924700e946b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Display the id of the RDD\n",
        "numbersRDD.id()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP39Q_4ERuOp"
      },
      "source": [
        "A name can be set to provide a more meaningful way of identifying an RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-89rZCSTRuOv",
        "outputId": "db0915e4-7712-4c7a-eb3c-fe2299d53a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range of integers\n"
          ]
        }
      ],
      "source": [
        "# Set the name of the RDD\n",
        "numbersRDD.setName('Range of integers')\n",
        "\n",
        "# Print the name of the RDD\n",
        "print(numbersRDD.name())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Pb6KwXRuOv"
      },
      "source": [
        "### Caching RDDs and simple actions\n",
        "Since we will be reusing the RDD many times, we ask Spark to cache the RDD in memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfCba5aURuOv",
        "outputId": "5f336fd0-8793-46b6-f61a-c667836b017f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Range of integers PythonRDD[1] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Cache the RDD\n",
        "numbersRDD.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYxMGKXRuOw"
      },
      "source": [
        "For small datasets, we can use collect() to retrieve and view the entire RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQHPbEZpRuOw",
        "outputId": "7e2f2bf7-3d05-4b86-93aa-608746efd951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Retrieve all the elements in the RDD to the driver program\n",
        "print(numbersRDD.collect())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MQOwc7jRuOx"
      },
      "source": [
        "Finally, let's verify that our RDD contains one hundred elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVoe7zhqRuOx",
        "outputId": "bbd11373-cf13-4d06-ecb7-6e49949385b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Count the number of elements in the RDD\n",
        "numbersRDD.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mj2UAgARuOy"
      },
      "source": [
        "### Simple transformations and actions\n",
        "(3a) Element-wise transformation using map\n",
        "\n",
        "We first look at map(), a transformation that applies a function to each element in the RDD. In this exercise, complete the function addOne, which increases an integer element by one. Following this, call map() on numbersRDD supplying the function addOne(). Notice how transformations do not mutate RDDs. Instead, they form new RDDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy6HJJFYRuOy"
      },
      "outputs": [],
      "source": [
        "# EXERCISE Replace <FILL IN> with the proper code\n",
        "\n",
        "def addOne(number):\n",
        "    \"\"\" Increases a number by one\n",
        "    Args:\n",
        "        number (int): an integer to increase\n",
        "    Returns:\n",
        "        int: the number increased by one\n",
        "    \"\"\"\n",
        "    return <FILL IN>\n",
        "\n",
        "numbersIncreasedRDD = numbersRDD.map(<FILL IN>)\n",
        "\n",
        "# RDDs are immutable\n",
        "print(\"The id of numbersRDD is:\", numbersRDD.id())\n",
        "print(\"The id of numbersIncreasedRDD is: \", numbersIncreasedRDD.id())\n",
        "# Verify that the range of numbers have been increased by one\n",
        "print(\"The RDD contains the numbers:\", numbersIncreasedRDD.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SGscyCyRuOz"
      },
      "source": [
        "### Lambda statements\n",
        "\n",
        "Next, repeat the same transformation, this time by supplying a lambda statement to map(). Lambda statements provide a convenient way of expressing short functions without defining a function body. A lambda statement takes a number of parameters and an expression, creating a function that returns the value of the expression: lambda parameters : expression(parameters)\n",
        "\n",
        "Next, repeat the transformation in (3a) using a lambda statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grb3pbY7RuOz"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the proper code\n",
        "\n",
        "# Increases each element by one using a lambda function\n",
        "numbersIncreasedRDD = numbersRDD.map(<FILL IN>)\n",
        "\n",
        "# Verify that the range of numbers have been increased by one\n",
        "print(numbersIncreasedRDD.collect())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxa0c1ZdRuOz"
      },
      "source": [
        "### Additional transformations\n",
        "\n",
        "Very often it is desirable to remove erroneous elements or elements not required for the desired calculations. filter(), takes a function and retains the elements satisfying the supplied function. Next, try filtering out all the elements not evenly divisible by 2 using the filter() transformation together with a lambda function. Supply filter() with a lambda function that returns True for every input divisible by 2 and False otherwise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkMv4JwwRuO0"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the a lambda function\n",
        "\n",
        "# Filters out all elements not evenly divisible by 2\n",
        "filteredNumbersRDD = numbersRDD.filter(<FILL IN>)\n",
        "\n",
        "# Print all elements evenly divisible by 2\n",
        "print(filteredNumbersRDD.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xytcB1LlRuO0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Test\n",
        "assert filteredNumbersRDD.count() == 50, \"The number of filtered elements is wrong!\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5TLHE-tRuO1"
      },
      "source": [
        "Some functions, such as range(), return lists of elements. When applied to individual elements in an RDD, these will create a nested structure, which depending on the application may be undesirable. In these cases, flatMap() can be useful in 'flattening' the resulting structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyRC0yYFRuO1"
      },
      "outputs": [],
      "source": [
        "nestedRDD = sc.parallelize([1,2,3])\n",
        "\n",
        "print(nestedRDD.map(lambda x:range(x)).collect())\n",
        "print(nestedRDD.flatMap(lambda x:range(x)).collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkFkyX3PRuO2"
      },
      "source": [
        "Consider the difference between using map() and flatMap(). Notice how the output from map() contains nested lists, while the output from flatMap() has been \"flattened\" to a single list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e47K7vAMRuO2"
      },
      "source": [
        "\n",
        "### Actions\n",
        "\n",
        "reduce() is a common action, which takes a function that operates on two elements and returns a new element of the same type. A common operation is to sum up the elements in an RDD using reduce(). Sum up the elements in the numbersRDD dataset. Lambda statements having more than one input element can be expressed as: lambda x1, x2, x3, ... : expression(x1, x2, x3, ...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmXPrGilRuO2"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the proper code\n",
        "\n",
        "# Sum up the elements in numbersRDD\n",
        "numbersSum = numbersRDD.reduce(<FILL IN>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bobx3XheRuO3"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "assert numbersSum == 5050, \"The sum is incorrect!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Few75ITmRuO3"
      },
      "source": [
        "In addition to using collect(), Spark provides a number of actions to retrieve a limited set of results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOesFXgdRuO3"
      },
      "outputs": [],
      "source": [
        "print(numbersRDD.take(5))\n",
        "print(numbersRDD.first())\n",
        "print(numbersRDD.top(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qmUiVE7RuO4"
      },
      "source": [
        "While the results from take(), first(), and top() differ from one run to another, takeOrdered() returns results in a deterministic way. takeOrdered() by default returns results in natural order. Additionally, a function may be supplied to change the ordering as desired. For instance, to a list of numbers in descending order, the numbers can simply be negated by a lambda function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHKNvnR5RuO4"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the proper code\n",
        "\n",
        "# Print the numbers in natural order\n",
        "print(numbersRDD.takeOrdered(5))\n",
        "\n",
        "# Supply a lambda function to return the elements in reversed order\n",
        "print(numbersRDD.takeOrdered(5 , <FILL IN>))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsu_BLgYRuO4"
      },
      "source": [
        "### Chaining expressions\n",
        "\n",
        "Since transformations return new RDDs, it is possible to chain several calls of operations together to form a pipeline. E.g. it is possible to express such a chain as: RDD.transformation1().transformation2().action(). Below we show two ways of chaining, both ways perform the same operations and provide a more readable code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBNgU9FxRuO4"
      },
      "outputs": [],
      "source": [
        "numbersFiltered = numbersRDD.map(lambda x : x + 1).filter(lambda x : x < 10).collect()\n",
        "\n",
        "\n",
        "numbersFiltered = (numbersRDD\n",
        "                   .map(lambda x : x + 1)\n",
        "                   .filter(lambda x : x < 10)\n",
        "                   .collect())\n",
        "\n",
        "print(numbersFiltered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov4m8oUQRuO5"
      },
      "source": [
        "### Now going back to the class example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzxhuDlUg7oP",
        "outputId": "a3df7677-a761-4a36-98b9-f3c04bde463c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path=\"./sample_data/\"\n",
        "sc = spark.sparkContext\n",
        "textFile = sc.textFile(file_path+\"README.md\")\n",
        "textFile.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PNOEX4sRuO5"
      },
      "outputs": [],
      "source": [
        "pythonlines = textFile.filter(lambda line:\"csv\" in line)\n",
        "pythonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_ABCUSLRuO6"
      },
      "outputs": [],
      "source": [
        "pythonlines.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYfsOaRcGToB"
      },
      "source": [
        "# WordCount\n",
        "In this final part of the exercise, we load a text file into Spark. We perform a simple tokenization of the text, splitting up lines to words. We remove punctuations, normalize the words, and remove empty elements to form an RDD of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiQf_D84Df1o",
        "outputId": "a66f7577-9429-4d23-da2f-bfc4a12964da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "directory: 1\n",
            "datasets: 1\n",
            "*: 3\n",
            "`california_housing_data*.csv`: 1\n",
            "is: 4\n",
            "housing: 1\n",
            "1990: 1\n",
            "more: 1\n",
            "at:: 2\n",
            "https://developers.google.com/machine-learning/crash-course/california-housing-data-description: 1\n",
            "`mnist_*.csv`: 1\n",
            "of: 2\n",
            "[Anscombe's: 1\n",
            "was: 2\n",
            "originally: 1\n",
            "in: 2\n",
            "Anscombe,: 1\n",
            "F.: 1\n",
            "'Graphs: 1\n",
            "American: 1\n",
            "Statistician.: 1\n",
            "(1):: 1\n",
            "prepared: 1\n",
            "This: 1\n",
            "includes: 1\n",
            "a: 3\n",
            "few: 1\n",
            "sample: 2\n",
            "to: 1\n",
            "get: 1\n",
            "you: 1\n",
            "started.: 1\n",
            "California: 1\n",
            "data: 1\n",
            "from: 1\n",
            "the: 3\n",
            "US: 1\n",
            "Census;: 1\n",
            "information: 1\n",
            "available: 1\n",
            "small: 1\n",
            "[MNIST: 1\n",
            "database](https://en.wikipedia.org/wiki/MNIST_database),: 1\n",
            "which: 1\n",
            "described: 2\n",
            "http://yann.lecun.com/exdb/mnist/: 1\n",
            "`anscombe.json`: 1\n",
            "contains: 1\n",
            "copy: 2\n",
            "quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet);: 1\n",
            "it: 1\n",
            "J.: 1\n",
            "(1973).: 1\n",
            "Statistical: 1\n",
            "Analysis'.: 1\n",
            "27: 1\n",
            "17-21.: 1\n",
            "JSTOR: 1\n",
            "2682899.: 1\n",
            "and: 1\n",
            "our: 1\n",
            "by: 1\n",
            "[vega_datasets: 1\n",
            "library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).: 1\n"
          ]
        }
      ],
      "source": [
        "wordCounts = textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
        "output=wordCounts.collect()\n",
        "for (word, count) in output:\n",
        "  print(\"%s: %i\" % (word, count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWShU9vRuO7"
      },
      "source": [
        "### Removing stop words\n",
        "\n",
        "In many cases when performing text analysis, it is often desirable to remove common words called 'stop words' such as 'the', 'a', and 'is'. Define a lambda function and apply a transformation that filters out the five stop words: 'the', 'and', 'i', 'to', and 'of'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NN5Fbk8RuO8"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the proper code\n",
        "\n",
        "filteredWordCount = wordCount.filter(<FILL IN>)\n",
        "\n",
        "print(filteredWordCount.count())\n",
        "\n",
        "print(filteredWordCount.take(30))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}