{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Notebook for Colab"
      ],
      "metadata": {
        "id": "aP2gGooHhilc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rosafilgueira/Seminar_MUIA.git"
      ],
      "metadata": {
        "id": "suX71c63hqZT",
        "outputId": "8c576901-f3de-4d41-b4c7-5f9fbe64a695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Seminar_MUIA'...\n",
            "remote: Enumerating objects: 331, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 331 (delta 55), reused 60 (delta 22), pack-reused 223\u001b[K\n",
            "Receiving objects: 100% (331/331), 43.94 MiB | 13.71 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "721YwCSph5IY",
        "outputId": "17fa3ef2-02bb-4f99-d798-c034285a57b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 49 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 44.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=99d0803d5c78d991ede834f5dbc3c49c16537bef9fe3964858148ed97d4ec713\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "UtmdnKd1h8eD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "meJpdPxPiCL7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp-GHcfFg0A1"
      },
      "source": [
        "\n",
        "# **Exercise 1: Spark basics & word count & Pi**\n",
        " This first exercise will introduce the basic Spark concepts and operations. We finish by performing a word count on a small text.\n",
        "\n",
        "The following material will be covered:\n",
        " - Part 1: Using the Jupyter notebook\n",
        " - Part 2: SparkContext\n",
        " - Part 3: Creating RDDs\n",
        " - Part 4: Simple transformations and actions\n",
        " - Part 5: Word count\n",
        "\n",
        "### We will look closer at the following Spark operations:\n",
        "* #### `parallelize()`, `persist()`, `collect()`, `count()`, `map()`, `flatMap()`, `filter()`, `reduce()`, `take()`, `takeOrdered()`, `first()`, `top()`, `textFile()`\n",
        "\n",
        "### During the exercises, the following resources might come in handy:\n",
        "*  Documentation of the [PySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)\n",
        "*  Documentation of the [Python API](https://docs.python.org/2.7/)\n",
        "\n",
        "### To run code in Jupyter, press: \n",
        "*  `Ctrl-Enter` to run the code in the currently selected cell\n",
        "*  `Shift-Enter` to run the code in the currently selected cell and jump to the next cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOkIprshg0A3"
      },
      "source": [
        "### **Part 1: Using the Jupyter notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmu_Fgjyg0A3"
      },
      "source": [
        " **(1a) Notebook usage and code execution**\n",
        "##### A Jupyter notebook is composed of a list of cells of different types. In the following exercises you will encounter two different types of cells: \n",
        "* A markdown cell, containing formatted text using a language called [Markdown](https://help.github.com/articles/markdown-basics/), such as this one you're reading now.\n",
        "* A code cell, containing executable code. Code cells have a grey background and show either input or output, marked by a beginning `\"In []\"` or `\"Out []\"` respectively.\n",
        "\n",
        "##### Code in a code cell can be executed using a number of ways:\n",
        "*  Pressing `Ctrl-Enter` runs the code in the currently selected cell\n",
        "*  Pressing `Shift-Enter` runs the code in the currently selected cell and jump to the next cell\n",
        "*  In addition to keyboard shortcuts, code can be executed by using the `Cell` menu or the `Play` icon in the menu bar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoRvl915g0A4"
      },
      "source": [
        " Next, try executing the code in the code cell below using one of the described methods. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3ymWGUUg0A4"
      },
      "outputs": [],
      "source": [
        "# Assign the value 43 to the variable a\n",
        "a = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K16I4nO5g0A4"
      },
      "source": [
        "The typical life cycle of a Spark program is –\n",
        "\n",
        "    Create RDDs from some external data source or parallelize a collection in your driver program.\n",
        "    Lazily transform the base RDDs into new RDDs using transformations.\n",
        "    Cache some of those RDDs for future reuse.\n",
        "    Perform actions to execute parallel computation and to produce results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq9EQytcg0A6"
      },
      "source": [
        "### **Part 2: Creating RDDs**\n",
        "The typical life cycle of a Spark program is:\n",
        "\n",
        "*  Create RDDs from some external data source or parallelize a collection. \n",
        "*  Lazily transform the base RDDs into new RDDs using transformations.\n",
        "*  Cache some of those RDDs for future reuse.\n",
        "*  Perform actions to execute parallel computation and to produce results.\n",
        "\n",
        "![imagen.png](attachment:imagen.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBZ9BoVtg0A6"
      },
      "source": [
        "#### **(2a) Creating a simple RDD**\n",
        " A simple way to create an RDD is to take an existing collection and load it into Spark by using the SparkContext's `parallelize()` method. We first start by creating a list of integers using Python's `xrange()` method. Following this, we create our first RDD by using the `parallelize()` method to load the list of numbers unto 8 partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOEFl1cwg0A6"
      },
      "outputs": [],
      "source": [
        "# Create a list of one hundred integers\n",
        "numbers = range(1, 101)\n",
        "\n",
        "# Create an RDD by dividing the list unto 8 partitions\n",
        "numbersRDD = sc.parallelize(numbers, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp2ZM5K1g0A7"
      },
      "source": [
        "Each RDD has a unique identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVcEsp7Ag0A8"
      },
      "outputs": [],
      "source": [
        "# Display the id of the RDD\n",
        "numbersRDD.id()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfsZFfpKg0A8"
      },
      "source": [
        " A name can be set to provide a more meaningful way of identifying an RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VH5ebXVg0A9"
      },
      "outputs": [],
      "source": [
        "# Set the name of the RDD\n",
        "numbersRDD.setName('Range of integers')\n",
        "\n",
        "# Print the name of the RDD\n",
        "print(numbersRDD.name())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U875wVy3g0A9"
      },
      "source": [
        "#### **(2b) Caching RDDs and simple actions**\n",
        " Since we will be reusing the RDD many times, we ask Spark to cache the RDD in memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6fb2OJtg0A9"
      },
      "outputs": [],
      "source": [
        "# Cache the RDD\n",
        "numbersRDD.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1OrPpV0g0A9"
      },
      "source": [
        " For small datasets, we can use `collect()` to retrieve and view the entire RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf_FzHN1g0A-"
      },
      "outputs": [],
      "source": [
        "# Retrieve all the elements in the RDD to the driver program\n",
        "print(numbersRDD.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPiw3MFPg0A-"
      },
      "source": [
        "Finally, let's verify that our RDD contains one hundred elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d9XAI_og0A-"
      },
      "outputs": [],
      "source": [
        "# Count the number of elements in the RDD\n",
        "numbersRDD.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHGdKaHCg0A-"
      },
      "source": [
        "### **Part 3: Simple transformations and actions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TbaCcOig0A-"
      },
      "source": [
        "#### **(3a) Element-wise transformation using map**\n",
        "We first look at `map()`, a transformation that applies a function to each element in the RDD. In this exercise, complete the function `addOne`, which increases an integer element by one. Following this, call `map()` on numbersRDD supplying the function `addOne()`. Notice how transformations do not mutate RDDs. Instead, they form new RDDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBY_Yu18g0A-"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the proper code\n",
        "\n",
        "def addOne(number):\n",
        "    \"\"\" Increases a number by one\n",
        "    Args:\n",
        "        number (int): an integer to increase\n",
        "    Returns:\n",
        "        int: the number increased by one\n",
        "    \"\"\"\n",
        "    return <FILL IN>\n",
        "\n",
        "numbersIncreasedRDD = numbersRDD.map(<FILL IN>)\n",
        "\n",
        "# RDDs are immutable\n",
        "print(\"The id of numbersRDD is:\", numbersRDD.id())\n",
        "print(\"The id of numbersIncreasedRDD is: \", numbersIncreasedRDD.id())\n",
        "# Verify that the range of numbers have been increased by one\n",
        "print(\"The RDD contains the numbers:\", numbersIncreasedRDD.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNlVeDdag0A_"
      },
      "source": [
        "#### **(3b) Lambda statements**\n",
        " Next, repeat the same transformation, this time by supplying a [lambda statement](https://docs.python.org/2.7/howto/functional.html#small-functions-and-the-lambda-expression) to `map()`. Lambda statements provide a convenient way of expressing short functions without defining a function body. A lambda statement takes a number of parameters and an expression, creating a function that returns the value of the expression: `lambda parameters : expression(parameters)`\n",
        " \n",
        " Next, repeat the transformation in (3a) using a lambda statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp3ciq4Vg0A_"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the proper code\n",
        "\n",
        "# Increases each element by one using a lambda function\n",
        "numbersIncreasedRDD = numbersRDD.map(<FILL IN>)\n",
        "\n",
        "# Verify that the range of numbers have been increased by one\n",
        "print(numbersIncreasedRDD.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1alS3Mbg0A_"
      },
      "source": [
        "#### **(3c) Additional transformations**\n",
        "Very often it is desirable to remove erroneous elements or elements not required for the desired calculations. `filter()`, takes a function and retains the elements satisfying the supplied function. Next, try filtering out all the elements not evenly divisible by 2 using the `filter()` transformation together with a lambda function. Supply `filter()` with a lambda function that returns `True` for every input divisible by 2 and `False` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icwjWixAg0BC"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the a lambda function\n",
        "\n",
        "# Filters out all elements not evenly divisible by 2\n",
        "filteredNumbersRDD = numbersRDD.filter(<FILL IN>)\n",
        "\n",
        "# Print all elements evenly divisible by 2\n",
        "print(filteredNumbersRDD.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8mQugslg0BC"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "assert filteredNumbersRDD.count() == 50, \"The number of filtered elements is wrong!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bT_k-CBg0BC"
      },
      "source": [
        "Some functions, such as range(), return lists of elements. When applied to individual elements in an RDD, these will create a nested structure, which depending on the application may be undesirable. In these cases, flatMap() can be useful in 'flattening' the resulting structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkIGYmavg0BD"
      },
      "outputs": [],
      "source": [
        "nestedRDD = sc.parallelize([1,2,3])\n",
        "\n",
        "print(nestedRDD.map(lambda x:range(x)).collect())\n",
        "print(nestedRDD.flatMap(lambda x:range(x)).collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSidC1QRg0BD"
      },
      "source": [
        " Consider the difference between using `map()` and `flatMap()`. Notice how the output from `map()` contains nested lists, while the output from `flatMap()` has been \"flattened\" to a single list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6TAkLF5g0BD"
      },
      "source": [
        "#### **(3d) Actions**\n",
        " `reduce()` is a common action, which takes a function that operates on two elements and returns a new element of the same type. A common operation is to sum up the elements in an RDD using `reduce()`.\n",
        " Sum up the elements in the numbersRDD dataset. Lambda statements having more than one input element can be expressed as: `lambda x1, x2, x3, ... : expression(x1, x2, x3, ...)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK08y-18g0BD"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the proper code\n",
        "\n",
        "# Sum up the elements in numbersRDD\n",
        "numbersSum = numbersRDD.reduce(<FILL IN>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIWzPnA9g0BD"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "assert numbersSum == 5050, \"The sum is incorrect!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k3NRx0eg0BD"
      },
      "source": [
        " In addition to using `collect()`, Spark provides a number of actions to retrieve a limited set of results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2q37l8Pg0BE"
      },
      "outputs": [],
      "source": [
        "print(numbersRDD.take(5))\n",
        "print(numbersRDD.first())\n",
        "print(numbersRDD.top(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IVCmrMzg0BE"
      },
      "source": [
        " While the results from `take()`, `first()`, and `top()` differ from one run to another, `takeOrdered()` returns results in a deterministic way. `takeOrdered()` by default returns results in natural order. Additionally, a function may be supplied to change the ordering as desired. For instance, to a list of numbers in descending order, the numbers can simply be negated by a lambda function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1a1xkAbg0BE"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the proper code\n",
        "\n",
        "# Print the numbers in natural order\n",
        "print(numbersRDD.takeOrdered(5))\n",
        "\n",
        "# Supply a lambda function to return the elements in reversed order\n",
        "print(numbersRDD.takeOrdered(5 , <FILL IN>))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ger0Yw6Kg0BE"
      },
      "source": [
        "#### **(3e) Chaining expressions**\n",
        "Since transformations return new RDDs, it is possible to chain several calls of operations together to form a pipeline. E.g. it is possible to express such a chain as: `RDD.transformation1().transformation2().action()`. Below we show two ways of chaining, both ways perform the same operations and provide a more readable code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHXJaMCOg0BE"
      },
      "outputs": [],
      "source": [
        "numbersFiltered = numbersRDD.map(lambda x : x + 1).filter(lambda x : x < 10).collect()\n",
        "\n",
        "\n",
        "numbersFiltered = (numbersRDD\n",
        "                   .map(lambda x : x + 1)\n",
        "                   .filter(lambda x : x < 10)\n",
        "                   .collect())\n",
        "\n",
        "print(numbersFiltered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsyp-pyeg0BE"
      },
      "source": [
        "#### **(3f) Lazy evaluation**\n",
        "Notice how quickly the transformation, `map()`, executes while the action, `count()`, takes longer. Spark defers execution of transformations until it encounters an action. This is called lazy evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOkb1mPWg0BE"
      },
      "outputs": [],
      "source": [
        "hugeNumbersRDD = sc.parallelize(range(1, 100000001), 8)\n",
        "hugeNumbersMultipliedRDD = hugeNumbersRDD.map(lambda x : x * 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guRZNyFpg0BF"
      },
      "outputs": [],
      "source": [
        "hugeNumbersMultipliedRDD.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vskM7mD2g0BF"
      },
      "source": [
        " Since Spark defers execution until it encounters an action, it can avoid making unnecessary computations. Since the action, `first()`, only requests the first element, Spark will avoid computation on the entire dataset. Notice the increase in speed compared to `count()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKOWbcV8g0BF"
      },
      "outputs": [],
      "source": [
        "hugeNumbersRDD = sc.parallelize(range(1, 100000001), 8)\n",
        "hugeNumbersMultipliedRDD = hugeNumbersRDD.map(lambda x : x * 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJdbuuUvg0BF"
      },
      "outputs": [],
      "source": [
        "hugeNumbersMultipliedRDD.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yllhYJIKg0BF"
      },
      "source": [
        "### **Part 4: Word count**\n",
        "In this final part of the exercise, we load a text file into Spark. We perform a simple tokenization of the text, splitting up lines to words. We remove punctuations, normalize the words, and remove empty elements to form an RDD of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CknmqCxg0BF"
      },
      "source": [
        "#### **(4a) loading text**\n",
        " Copy all-shakespeare.txt to HDFS as shakespeare.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcCLKsDLg0BF"
      },
      "source": [
        "\n",
        " Load our textfile into an RDD. Then we split the words by spaces, creat a key-value (word-1) for each word, and then we reduce the number of each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4681DW9g0BF"
      },
      "outputs": [],
      "source": [
        "textRDD = sc.textFile('shakespeare.txt', 8)\n",
        "textRDD.take(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVYP2JJQg0BG"
      },
      "outputs": [],
      "source": [
        "\n",
        "wordCount = textRDD.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxj13Xh1g0BG"
      },
      "source": [
        "#### **(5b) Removing stop words**\n",
        " In many cases when performing text analysis, it is often desirable to remove common words called 'stop words' such as 'the', 'a', and 'is'. Define a lambda function and apply a transformation that filters out the five stop words: 'the', 'and', 'i', 'to', and 'of'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF5SyNlPg0BG"
      },
      "outputs": [],
      "source": [
        "# Replace <FILL IN> with the proper code\n",
        "\n",
        "filteredWordCount = wordCount.filter(<FILL IN>)\n",
        "\n",
        "print(filteredWordCount.count())\n",
        "\n",
        "print(filteredWordCount.take(30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdPb1Eikg0BH"
      },
      "source": [
        "### **Part 5: PI Estimation**\n",
        " Spark can also be used for compute-intensive tasks. This code estimates (montecarlo method) π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISw9NO3Kg0BH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "num_samples = 10000\n",
        "\n",
        "def inside(p):\n",
        "  x, y = random.random(), random.random()\n",
        "  return x*x + y*y < 1\n",
        "\n",
        "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
        "\n",
        "pi = 4 * count / num_samples\n",
        "print(pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTnhm2fKg0BH"
      },
      "source": [
        "For clarification what this Spark code does, I have included the classic python version. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spGLQCdSg0BH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        " \n",
        "def inside(x,y):\n",
        "    if(x**2+y**2<1):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        " \n",
        "\n",
        "circleArea = 0\n",
        "num_samples = 10000\n",
        "pi = 0\n",
        "for i in range(0,num_samples):\n",
        "    x = random.random()\n",
        "    y = random.random()\n",
        "    \n",
        "    if(inside(x,y)==1):\n",
        "        circleArea=circleArea+1\n",
        "        \n",
        "pi = 4.0*circleArea/num_samples\n",
        "print(\"Approximate value for pi: \", pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQy_P4psg0BH"
      },
      "source": [
        "### Conclusion\n",
        "In the next exercise we will revisit word count and count the number of occurrences of each individual word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ-uhoZYg0BH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2"
      ],
      "metadata": {
        "id": "19D8wZ-1ioAA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "lab1_basics.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}