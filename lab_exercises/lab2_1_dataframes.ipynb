{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 2: Spark SQL**\n",
    " This second exercise will introduce database operations with Spark. \n",
    "\n",
    "\n",
    "##### During the exercises, the following resources might come in handy:\n",
    "*  Documentation of the [PySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)\n",
    "*  Documentation of the [Python API](https://docs.python.org/2.7/)\n",
    "*  Documentation of the [Spark SQL API](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "*  Documentation of [Hive SQL](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)\n",
    "\n",
    "##### To run code in Jupyter, press: \n",
    "*  `Ctrl-Enter` to run the code in the currently selected cell\n",
    "*  `Shift-Enter` to run the code in the currently selected cell and jump to the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Helper: Displays rows from a Spark SQL object as HTML**\n",
    " The following code can display arrays of rows or dataframes as a html table. This creates a human-friendly output of our data. It is enough to browse through this code, as it is not important to know it in detail for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "\n",
    "def displayRows(rowDf):\n",
    "    headers = []\n",
    "    rows = []\n",
    "    if(str(type(rowDf)) == \"<class 'pyspark.sql.dataframe.DataFrame'>\"):\n",
    "        rows = rowDf.limit(10000).collect() #Let's limit the output just in case!\n",
    "        if(len(rows) == 10000):\n",
    "            if(rowDf.limit(10001).count() == 10001):\n",
    "                warnings.warn(\"More than 10 000 rows was returned, only showing the first 10 000.\")\n",
    "                \n",
    "        headers = list(rowDf.columns)\n",
    "    else:\n",
    "        rows = rowDf\n",
    "        if(len(rows) > 10000):\n",
    "            warnings.warn(\"Rows has {0} elements, only showing the first 10 000.\".format(len(rows)))\n",
    "            rows = rows[0:10000]\n",
    "            \n",
    "        #Computes the unique set of keys\n",
    "        headers = list(sorted(reduce(lambda x,y: x.union(set(y.asDict().iterkeys())), rows, set())))\n",
    "            \n",
    "    tableHead = [\"<th>{0}</th>\".format(key) for key in headers]\n",
    "    tableBody = [\"<tr>{0}</tr>\".format(\n",
    "                    \"\".join([\"<td>{0}</td>\".format(rowDict.get(header)) \n",
    "                            for rowDict \n",
    "                            in (row.asDict(),) \n",
    "                            for header \n",
    "                            in headers])\n",
    "                    ) for row in rows]\n",
    "    \n",
    "    display(HTML(\n",
    "    u\"\"\"<table>\n",
    "    <thead><tr>{0}</tr></thead>\n",
    "    <tbody>{1}</tbody>\n",
    "    </table>\n",
    "    \"\"\".format(\"\".join(tableHead), \"\".join(tableBody))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Learning Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This part will introduce you the Spark SQL by writing SQL queries.\n",
    "\n",
    "The cell below generates data which you will write queries for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'apple'), (2, 'banana'), (3, 'grapes'), (4, 'pear')]\n"
     ]
    }
   ],
   "source": [
    "my_list = ['apple', 'banana', 'grapes', 'pear']\n",
    "counter_list = list(enumerate(my_list, 1))\n",
    "print(counter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#Top 20 boy and girl names 2014 in random order.\n",
    "names = [\"Caden\", \"Kaylee\", \"Lucas\", \"Ethan\", \"Alexander\", \"Jackson\", \n",
    "         \"Aiden\", \"Madelyn\", \"Michael\", \"Avery\", \"Luke\", \"Isabella\", \n",
    "         \"Chloe\", \"Elijah\", \"Abigail\", \"Madison\", \"Jacob\", \"Zoe\", \"Emily\", \n",
    "         \"Jayden\", \"Liam\", \"Mason\", \"Mia\", \"Sophia\", \"Benjamin\", \"Layla\", \n",
    "         \"Emma\", \"Lily\", \"Charlotte\", \"Caleb\", \"James\", \"Noah\", \"Ella\", \n",
    "         \"Jack\", \"Jayce\", \"Aubrey\", \"Olivia\", \"Harper\", \"Logan\", \"Ava\"]\n",
    "#A-G in phonetic alphabet\n",
    "groups = [\"Alpha\",\"Bravo\", \"Charlie\", \"Delta\", \"Echo\", \"Foxtrot\", \"Golf\"]\n",
    "\n",
    "#Some numeric magic to generate not so uniform random data.\n",
    "tblUserRdd = sc.parallelize(map(lambda i: (i, ((i*104729)^131) % 7, 26500 + ((i*104729)^96587) % 6367), range(1,51)))\n",
    "# Parallelize data using 4 partitions\n",
    "tblNamesRdd = sc.parallelize(enumerate(names, 1), 4)\n",
    "# Parallelize data using 2 partitions\n",
    "tblGroupNamesRdd = sc.parallelize(enumerate(groups), 2)\n",
    "\n",
    "#Create dataframes from the RDDs\n",
    "tblNames      = sqlContext.createDataFrame(tblNamesRdd,      [\"userId\", \"name\"])\n",
    "tblUsers      = sqlContext.createDataFrame(tblUserRdd,       [\"id\", \"groupId\", \"salary\"])\n",
    "tblGroupNames = sqlContext.createDataFrame(tblGroupNamesRdd, [\"id\", \"name\"])\n",
    "\n",
    "#Register them for use with SQL-TRADITIONAL.\n",
    "sqlContext.registerDataFrameAsTable(tblGroupNames, \"tblGroupNames\")\n",
    "sqlContext.registerDataFrameAsTable(tblUsers, \"tblUsers\")\n",
    "sqlContext.registerDataFrameAsTable(tblNames, \"tblNames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets get some basic information about each dataframe\n",
    "\n",
    "Dataframes are structured meaning that types and columns are well-defined; if you have read the data generation cell you might have noticed that the types were not specified. These are inferred by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes provide a very handy function called `printSchema()`. As its name implies, it shows the schema of the data, including column names and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- groupId: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tblUsers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to call a number of operations on dataframe, similar to RDDs dataframes have a `count()` action to display the number of rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblUsers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tblGroupNames.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblGroupNames.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- groupId: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tblUsers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblNames.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next 3 cells will display the content of the dataframe by using the helper function *displayRows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><th>id</th><th>groupId</th><th>salary</th></tr></thead>\n",
       "    <tbody><tr><td>1</td><td>5</td><td>26623</td></tr><tr><td>2</td><td>5</td><td>30452</td></tr><tr><td>3</td><td>5</td><td>30462</td></tr><tr><td>4</td><td>6</td><td>27932</td></tr><tr><td>5</td><td>6</td><td>26973</td></tr><tr><td>6</td><td>2</td><td>32796</td></tr><tr><td>7</td><td>2</td><td>29202</td></tr><tr><td>8</td><td>3</td><td>32531</td></tr><tr><td>9</td><td>3</td><td>28969</td></tr><tr><td>10</td><td>3</td><td>29034</td></tr><tr><td>11</td><td>0</td><td>27978</td></tr><tr><td>12</td><td>1</td><td>30759</td></tr><tr><td>13</td><td>1</td><td>31825</td></tr><tr><td>14</td><td>1</td><td>28231</td></tr><tr><td>15</td><td>1</td><td>30599</td></tr><tr><td>16</td><td>5</td><td>29023</td></tr><tr><td>17</td><td>5</td><td>32820</td></tr><tr><td>18</td><td>5</td><td>32862</td></tr><tr><td>19</td><td>5</td><td>30292</td></tr><tr><td>20</td><td>6</td><td>29373</td></tr><tr><td>21</td><td>3</td><td>28285</td></tr><tr><td>22</td><td>3</td><td>28018</td></tr><tr><td>23</td><td>3</td><td>28156</td></tr><tr><td>24</td><td>4</td><td>31849</td></tr><tr><td>25</td><td>4</td><td>30922</td></tr><tr><td>26</td><td>0</td><td>30346</td></tr><tr><td>27</td><td>0</td><td>26784</td></tr><tr><td>28</td><td>1</td><td>27954</td></tr><tr><td>29</td><td>1</td><td>30631</td></tr><tr><td>30</td><td>1</td><td>26600</td></tr><tr><td>31</td><td>5</td><td>31911</td></tr><tr><td>32</td><td>6</td><td>28341</td></tr><tr><td>33</td><td>6</td><td>29503</td></tr><tr><td>34</td><td>6</td><td>32180</td></tr><tr><td>35</td><td>6</td><td>32245</td></tr><tr><td>36</td><td>3</td><td>30685</td></tr><tr><td>37</td><td>3</td><td>30386</td></tr><tr><td>38</td><td>3</td><td>30556</td></tr><tr><td>39</td><td>3</td><td>27858</td></tr><tr><td>40</td><td>4</td><td>26923</td></tr><tr><td>41</td><td>1</td><td>32234</td></tr><tr><td>42</td><td>1</td><td>29664</td></tr><tr><td>43</td><td>1</td><td>29834</td></tr><tr><td>44</td><td>2</td><td>29415</td></tr><tr><td>45</td><td>2</td><td>28488</td></tr><tr><td>46</td><td>2</td><td>27784</td></tr><tr><td>47</td><td>5</td><td>30717</td></tr><tr><td>48</td><td>6</td><td>31903</td></tr><tr><td>49</td><td>6</td><td>28181</td></tr><tr><td>50</td><td>6</td><td>28278</td></tr></tbody>\n",
       "    </table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayRows(tblUsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><th>userId</th><th>name</th></tr></thead>\n",
       "    <tbody><tr><td>1</td><td>Caden</td></tr><tr><td>2</td><td>Kaylee</td></tr><tr><td>3</td><td>Lucas</td></tr><tr><td>4</td><td>Ethan</td></tr><tr><td>5</td><td>Alexander</td></tr><tr><td>6</td><td>Jackson</td></tr><tr><td>7</td><td>Aiden</td></tr><tr><td>8</td><td>Madelyn</td></tr><tr><td>9</td><td>Michael</td></tr><tr><td>10</td><td>Avery</td></tr><tr><td>11</td><td>Luke</td></tr><tr><td>12</td><td>Isabella</td></tr><tr><td>13</td><td>Chloe</td></tr><tr><td>14</td><td>Elijah</td></tr><tr><td>15</td><td>Abigail</td></tr><tr><td>16</td><td>Madison</td></tr><tr><td>17</td><td>Jacob</td></tr><tr><td>18</td><td>Zoe</td></tr><tr><td>19</td><td>Emily</td></tr><tr><td>20</td><td>Jayden</td></tr><tr><td>21</td><td>Liam</td></tr><tr><td>22</td><td>Mason</td></tr><tr><td>23</td><td>Mia</td></tr><tr><td>24</td><td>Sophia</td></tr><tr><td>25</td><td>Benjamin</td></tr><tr><td>26</td><td>Layla</td></tr><tr><td>27</td><td>Emma</td></tr><tr><td>28</td><td>Lily</td></tr><tr><td>29</td><td>Charlotte</td></tr><tr><td>30</td><td>Caleb</td></tr><tr><td>31</td><td>James</td></tr><tr><td>32</td><td>Noah</td></tr><tr><td>33</td><td>Ella</td></tr><tr><td>34</td><td>Jack</td></tr><tr><td>35</td><td>Jayce</td></tr><tr><td>36</td><td>Aubrey</td></tr><tr><td>37</td><td>Olivia</td></tr><tr><td>38</td><td>Harper</td></tr><tr><td>39</td><td>Logan</td></tr><tr><td>40</td><td>Ava</td></tr></tbody>\n",
       "    </table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayRows(tblNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><th>id</th><th>name</th></tr></thead>\n",
       "    <tbody><tr><td>0</td><td>Alpha</td></tr><tr><td>1</td><td>Bravo</td></tr><tr><td>2</td><td>Charlie</td></tr><tr><td>3</td><td>Delta</td></tr><tr><td>4</td><td>Echo</td></tr><tr><td>5</td><td>Foxtrot</td></tr><tr><td>6</td><td>Golf</td></tr></tbody>\n",
       "    </table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayRows(tblGroupNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a basic function for displaying the contents of an Dataframe by using *show()*\n",
    "However, the output is limited and gives a limited view of a long column. It is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|groupId|salary|\n",
      "+---+-------+------+\n",
      "|  1|      5| 26623|\n",
      "|  2|      5| 30452|\n",
      "|  3|      5| 30462|\n",
      "|  4|      6| 27932|\n",
      "|  5|      6| 26973|\n",
      "|  6|      2| 32796|\n",
      "|  7|      2| 29202|\n",
      "|  8|      3| 32531|\n",
      "|  9|      3| 28969|\n",
      "| 10|      3| 29034|\n",
      "| 11|      0| 27978|\n",
      "| 12|      1| 30759|\n",
      "| 13|      1| 31825|\n",
      "| 14|      1| 28231|\n",
      "| 15|      1| 30599|\n",
      "| 16|      5| 29023|\n",
      "| 17|      5| 32820|\n",
      "| 18|      5| 32862|\n",
      "| 19|      5| 30292|\n",
      "| 20|      6| 29373|\n",
      "+---+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tblUsers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the first query you will write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.a) Write a query that selects all user ids in the group with id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nmismatched input '<' expecting {<EOF>, ';'}(line 4, pos 6)\n\n== SQL ==\n\nSELECT id \nFROM tblUsers \nWHERE <FILL IN>\n------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace <FILL IN> with the proper code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m q1a \u001b[38;5;241m=\u001b[39m \u001b[43msqlContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43mSELECT id \u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43mFROM tblUsers \u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43mWHERE <FILL IN>\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m displayRows(q1a)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/context.py:437\u001b[0m, in \u001b[0;36mSQLContext.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \nmismatched input '<' expecting {<EOF>, ';'}(line 4, pos 6)\n\n== SQL ==\n\nSELECT id \nFROM tblUsers \nWHERE <FILL IN>\n------^^^\n"
     ]
    }
   ],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "q1a = sqlContext.sql(\"\"\"\n",
    "SELECT id \n",
    "FROM tblUsers \n",
    "WHERE <FILL IN>\n",
    "\"\"\")\n",
    "\n",
    "displayRows(q1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(map(lambda row: row.id, q1a.collect())) == set([11,26,27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.b) Write a query that finds the min and max userId grouped by groupId\n",
    "\n",
    "The result should have the following columns:\n",
    "\n",
    "1. minUserId: The min user id per group\n",
    "2. maxUserId: The max user id per group\n",
    "2. groupId: The group id\n",
    "\n",
    "**Hint:** Use GROUP BY, MIN, MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1b = sqlContext.sql(\"\"\"\n",
    "SELECT \n",
    "    <FILL IN> AS minUserId, \n",
    "    <FILL IN> AS maxUserId,\n",
    "    <FILL IN> \n",
    "FROM tblUsers \n",
    "<FILL IN>\n",
    "\"\"\")\n",
    "\n",
    "displayRows(q1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minIds = {0: 11,\n",
    " 1: 12,\n",
    " 2: 6,\n",
    " 3: 8,\n",
    " 4: 24,\n",
    " 5: 1,\n",
    " 6: 4}\n",
    "\n",
    "maxIds = {0: 27,\n",
    " 1: 43,\n",
    " 2: 46,\n",
    " 3: 39,\n",
    " 4: 40,\n",
    " 5: 47,\n",
    " 6: 50}\n",
    "\n",
    "assert all(map(lambda row: minIds[row.groupId] == row.minUserId, q1b.collect()))\n",
    "assert all(map(lambda row: maxIds[row.groupId] == row.maxUserId, q1b.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.c) Compute the global average salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you do not specify any group by columns and use aggregating functions such as **AVG**(column) then the aggregation will be performed over the entire result and return a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSalary = sqlContext.sql(\"\"\"\n",
    "SELECT <FILL IN> AS avgSalary \n",
    "FROM tblUsers\n",
    "\"\"\").collect()[0].avgSalary\n",
    "\n",
    "avgSalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert avgSalary == 29707.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.d) Aggregate salaries per group\n",
    "\n",
    "Group per groupId and compute the minimum, average, maximum salary and sort by average salary descending.\n",
    "\n",
    "**Hint:** Use MIN, AVG, MAX, GROUP BY, you can sort by computed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1d = sqlContext.sql(\"\"\"\n",
    "SELECT \n",
    "    groupId,\n",
    "    COUNT(id) AS NumUsers,\n",
    "    <FILL IN> AS MinSalary, \n",
    "    <FILL IN> AS AvgSalary,\n",
    "    <FILL IN> AS MaxSalary,\n",
    "    AVG(salary) - {} AS GlobalAvgDelta\n",
    "FROM tblUsers\n",
    "<FILL IN>\n",
    "ORDER BY <FILL IN>\n",
    "\"\"\".format(avgSalary))\n",
    "\n",
    "displayRows(q1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (5, 26623, 30573, 32862), \n",
    "    (4, 26923, 29898, 31849),\n",
    "    (1, 26600, 29833, 32234),\n",
    "    (2, 27784, 29537, 32796),\n",
    "    (6, 26973, 29490, 32245),\n",
    "    (3, 27858, 29447, 32531),\n",
    "    (0, 26784, 28369, 30346)\n",
    "]\n",
    "\n",
    "q1dresult = q1d.collect()\n",
    "assert len(q1dresult) == 7\n",
    "assert map(lambda i: q1dresult[i].groupId == groups[i][0], range(0,len(q1dresult)-1)), \"GroupID column does not match.\"\n",
    "assert map(lambda i: q1dresult[i].MinSalary == groups[i][1], range(0,len(q1dresult)-1)), \"MinSalary column does not match.\"\n",
    "assert map(lambda i: int(q1dresult[i].AvgSalary) == groups[i][2], range(0,len(q1dresult)-1)), \"AvgSalary column does not match.\"\n",
    "assert map(lambda i: q1dresult[i].MaxSalary == groups[i][3], range(0,len(q1dresult)-1)), \"MaxSalary column does not match.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
