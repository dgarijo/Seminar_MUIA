{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframes (and RDDs) for manipulating text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your cirrus account, in the login node, type the following commands, for installing different nltk packages that we will use later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[XXX@cirrus-login0 lab_exercises]$ module load anaconda/python3\n",
    "[XXX@cirrus-login0 lab_exercises]$ python\n",
    "Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) \n",
    "[GCC 7.2.0] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import nltk\n",
    ">>> nltk.download('wordnet')\n",
    ">>> nltk.download('punkt')\n",
    ">>> nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise is to learn how to manipulate data using spark dataframes. We are going to work with a subset of the encyclopaedia britannica from the [National Library of Scotland](https://data.nls.uk/data/digitised-collections/encyclopaedia-britannica/). We have downloaded previously the full dataset, ingested the data, and produce a subsample dataset, called *\"nls_demo.csv\"* (which is the one we are going to use) using a spark-tool called [defoe](https://github.com/alan-turing-institute/defoe/blob/master/docs/nls_demo_examples/nls_demo_individual_queries.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the format of the CSV file:\n",
    "title,edition,year,place,archive_filename,page_filename,page_id,num_pages,type_archive,model,preprocess,page_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file into a dataframe\n",
    "df= sqlContext.read.csv(\"/lustre/home/shared/y15/spark/data/nls_demo.csv\", header=\"true\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering by pages that are not null, and grouping by year, and counting the number of pages\n",
    "df.filter(df.page_string.isNotNull()).select(df.year, df.page_string).groupby(df.year).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many rows do we have with the value \"year\" in the column \"year\"\n",
    "df[df.year.like(\"year\")].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter again the data, which pages are not null, and which years are not \"year\", and selecting 2 columns, and counting the elements by year\n",
    "df.filter(df.page_string.isNotNull()).filter(df[\"year\"]!=\"year\").select(df.year, df.page_string).groupby(df.year).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same that before, but grouping by place\n",
    "df.filter(df.page_string.isNotNull()).filter(df[\"year\"]!=\"year\").select(df.place, df.page_string).groupby(df.place).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping by years, but just the ones between 1773 and 1842\n",
    "df.filter(df.page_string.isNotNull()).filter(df[\"year\"]!=\"year\").filter(df.year.between(1773, 1842)).select(df.place, df.edition, df.page_string, df.year).groupby(df.year).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.page_string.isNotNull()).filter(df[\"year\"]!=\"year\").filter(df.year.between(1773, 1842)).select(df.place, df.edition, df.page_string, df.year).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.page_string.isNotNull()).filter(df[\"year\"]!=\"year\").filter(df.year.between(1773, 1842)).filter(df.edition.startswith(\"Second\")).select(df.place, df.edition, df.page_string, df.year).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a datafame, which pages are not Null, and selecting just year and page_string columns\n",
    "newdf=df.filter(df.page_string.isNotNull()).select(df.year, df.page_string)\n",
    "# And check the Schema of the new dataframe\n",
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows\n",
    "newdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 20 rows\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dataframe to tuples- best suited for processing unstructured data.\n",
    "pages=newdf.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages.take(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_TokenizeFunct(x):\n",
    "    print (\"%s\" %x)\n",
    "    return nltk.sent_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceTokenizeRDD = pages.map(lambda p: sent_TokenizeFunct(p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceTokenizeRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_TokenizeFunct(x):\n",
    "    splitted = [word for line in x for word in line.split()]\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokenizeRDD = sentenceTokenizeRDD.map(word_TokenizeFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokenizeRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWordsFunct(x):\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filteredSentence = [w for w in x if not w in stop_words]\n",
    "    return filteredSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopwordRDD = wordTokenizeRDD.map(removeStopWordsFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuationsFunct(x):\n",
    "    nltk.download('punkt')\n",
    "    list_punct=list(string.punctuation)\n",
    "    filtered = [''.join(c for c in s if c not in list_punct) for s in x] \n",
    "    filtered_space = [s for s in filtered if s] #remove empty space \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmvPunctRDD = stopwordRDD.map(removePunctuationsFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmvPunctRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizationFunct(x):\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    finalLem = [lemmatizer.lemmatize(s) for s in x]\n",
    "    return finalLem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_wordsRDD = rmvPunctRDD.map(lemmatizationFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_wordsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
