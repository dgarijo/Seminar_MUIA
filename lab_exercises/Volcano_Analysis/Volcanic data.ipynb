{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## This notebook is for analysing VAAs - Volcanic Ash Advisories reports## \n",
    "\n",
    "Information   issued by a   Volcanic   Ash   Advisory   Center (VAAC)  concerning  the occurrence or expected  occurrence of volcanic  ash  that  may  affect the safety of aircraft operations. A VAA is a text message that identifies the volcano, time of eruption, observed position of the ash cloud, and the forecasted position of the ash. The\n",
    "VAA is not to be used as a warning message. \n",
    "\n",
    "In the file *vaac_total_line.json* we can find information from many VAAs emited by different VAACs ( one entry by report).  We are going to a _DataFame_ to load the data and get the schema.\n",
    "\n",
    "And we can explore/analyse different properties of the dataset:\n",
    "    * Visualize/analyze the colours_code (warning level) by selecting a particular volcano.   \n",
    "    * Number of reports for a particular volcano\n",
    "    * Plot the flight level of the ashes observed and erupted (e.g. fl_obs_cld_L1, fl_eruption_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaacDF = spark.read.json('vaac_total_line.json')\n",
    "vaacDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaacDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create a new dataframe (df) by filtering out the rows which we dont have an issued date. And later from these rows, we are going to visualize the id of the volcano, colour and date values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, to_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "df = (vaacDF \n",
    "        .where(col('issued_date').isNotNull())\n",
    "        .select('v_id','name', 'colour_code', concat('issued_date', 'issued_time').alias('date'))\n",
    "        .withColumn('date', to_timestamp('date', 'yyyyMMddHHmm').cast(TimestampType()).alias('date'))\n",
    "     )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to create another dataframe (codedf) by replacing red, orange, yellow and green  by  \"4\",\"3\",\"2\",\"1\" respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "codedf = (df\n",
    "              .where(col('colour_code').isNotNull())\n",
    "              .replace(['red', 'orange', 'yellow', 'green'], [\"4\", \"3\", \"2\", \"1\"], 'colour_code')\n",
    "              .withColumn('colour_code', col('colour_code').cast(IntegerType()))\n",
    "         )\n",
    "codedf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to visualise all the different volcano ids (v_id) that we have in the codedf dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = codedf.select('v_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a pandas dataframe (data) from the codedf dataframe. In this dataframe we will store the v_ids which are equal 252010 (so we select a particular volcano), and we will store the date and colour_code information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "data = codedf.where('v_id == \"252010\"').select('date', 'colour_code').toPandas()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print the dataframe for visualzing the change of colours over the years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(data['date'].tolist(), data['colour_code'].tolist())\n",
    "plt.yticks(range(0,5), ('', 'green', 'yellow', 'orange', 'red') )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also analyse the frequency of reports of a particu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = vaacDF.filter('v_id == 283040').select('v_id','issued_date').groupBy('issued_date').count()\n",
    "fd = frequency.toPandas()\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = fd.plot.bar('issued_date', 'count')\n",
    "ax.set_title('Frequency of vaac reports for 283040')\n",
    "ax.legend(numpoints=1, loc='upper left')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Frequency', color='g')\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
